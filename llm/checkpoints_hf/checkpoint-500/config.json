{
  "architectures": [
    "GPTForCausalLM"
  ],
  "block_size": 256,
  "dropout": 0.2,
  "model_type": "custom_gpt",
  "moe_loss_coeff": 0.01,
  "n_embd": 768,
  "n_experts": 3,
  "n_head": 12,
  "n_layer": 8,
  "sliding_window_size": 64,
  "torch_dtype": "float32",
  "transformers_version": "4.48.3",
  "use_moe": true,
  "vocab_size": 65
}
