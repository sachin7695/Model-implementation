{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6411127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math \n",
    "import torch.functional as F \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022ec133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255,  0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679, -1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398, -0.9211,  1.5433],\n",
      "         [ 1.3488, -0.1396,  0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931,  1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627, -1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016,  1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513,  1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340, -1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239,  0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718, -0.5974, -0.6937],\n",
      "         [ 1.6455, -0.8030,  1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048,  2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103,  0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333,  1.6097, -0.4032]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "x = torch.randn(size=(2,8,4)) #bs, T, dim 8 tokens \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d7270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.1808, -0.0700, -0.3596, -0.9152],\n",
      "          [ 0.6258,  0.0255,  0.9545,  0.0643]],\n",
      "\n",
      "         [[ 0.3612,  1.1679, -1.3499, -0.5102],\n",
      "          [ 0.2360, -0.2398, -0.9211,  1.5433]],\n",
      "\n",
      "         [[ 1.3488, -0.1396,  0.2858,  0.9651],\n",
      "          [-2.0371,  0.4931,  1.4870,  0.5910]],\n",
      "\n",
      "         [[ 0.1260, -1.5627, -1.1601, -0.3348],\n",
      "          [ 0.4478, -0.8016,  1.5236,  2.5086]]],\n",
      "\n",
      "\n",
      "        [[[-0.6631, -0.2513,  1.0101,  0.1215],\n",
      "          [ 0.1584,  1.1340, -1.1539, -0.2984]],\n",
      "\n",
      "         [[-0.5075, -0.9239,  0.5467, -1.4948],\n",
      "          [-1.2057,  0.5718, -0.5974, -0.6937]],\n",
      "\n",
      "         [[ 1.6455, -0.8030,  1.3514, -0.2759],\n",
      "          [-1.5108,  2.1048,  2.7630, -1.7465]],\n",
      "\n",
      "         [[ 1.4516, -1.5103,  0.8212, -0.2115],\n",
      "          [ 0.7789,  1.5333,  1.6097, -0.4032]]]])\n",
      "\n",
      "tensor([ 0.1808, -0.0700, -0.3596, -0.9152]) tensor([0.6258, 0.0255, 0.9545, 0.0643])\n"
     ]
    }
   ],
   "source": [
    "def split_into_windows(x, window_size=2):\n",
    "    bs, seq_len, dim = x.shape \n",
    "    num_windows = seq_len // window_size \n",
    "    x = x.view(bs, num_windows, window_size, dim)\n",
    "    return x\n",
    "y = split_into_windows(x)\n",
    "print(y)\n",
    "print()\n",
    "batch_1 = y[0]\n",
    "win_1_batch_1 = y[0][0]\n",
    "chunk_1_win_1_batch_1 = y[0][0][0]\n",
    "chunk_2_win_1_batch_1 = y[0][0][1]\n",
    "print(chunk_1_win_1_batch_1,chunk_2_win_1_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "255fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalWindowAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 window_size = 512,\n",
    "                 embedding_dim = 768,\n",
    "                 num_attention_heads = 12,\n",
    "                 causal = False,\n",
    "                 look_backward = 1,\n",
    "                 look_forward=1,\n",
    "                 attention_dropout = 0.0\n",
    "                ):\n",
    "        \n",
    "\n",
    "        super(LocalWindowAttention, self).__init__()\n",
    "        self.causal = causal\n",
    "        self.look_backward = look_backward\n",
    "        self.look_forward = look_forward \n",
    "        self.window_size = window_size\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.num_heads = num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads \n",
    "\n",
    "        # Projections\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def split_into_windows(self, x):\n",
    "        if x.dim()==4:\n",
    "            b, h, seq_len, head_dim = x.shape\n",
    "            num_windows = seq_len // self.window_size\n",
    "            x = x.view(b, h, num_windows, self.window_size, head_dim) \n",
    "            return x\n",
    "    \n",
    "        b, seq_len, d = x.shape \n",
    "        num_windows = seq_len // self.window_size \n",
    "        x = x.view(b, num_windows, self.window_size, d) \n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def collect_windows(self, x, backward =1, forward = 1, pad_value = -1):\n",
    "        if x.dim() == 4:\n",
    "            batch_heads, num_windows, window_size, embed_dim = x.shape \n",
    "            pad = (0, 0, 0, 0, backward, forward)\n",
    "\n",
    "        elif x.dim()==3:\n",
    "            bath_head_dim, num_windows, window_size = x.shape \n",
    "\n",
    "            pad = (0, 0, backward, forward)\n",
    "\n",
    "        x = nn.functional.pad(x, pad=pad, value=pad_value)\n",
    "        gathered = []\n",
    "\n",
    "        for i in range(num_windows): #2 windows\n",
    "            start_idx = i \n",
    "            end_idx = i+forward+backward \n",
    "\n",
    "            grabbed_win = x[:, start_idx:end_idx+1] #[[pad], W0, W1] bs, win=3, window_size, embed_dim\n",
    "            grabbed_win = grabbed_win.flatten(1, 2).unsqueeze(1) #bs, 1, 3*window_size, embed_dim\n",
    "\n",
    "            gathered.append(grabbed_win) \n",
    "        gathered = torch.cat(gathered, dim=1) #bs, num_win, 3*win_size, embed_dim \n",
    "        ''' \n",
    "        earlier we were having for each window window_size tokens \n",
    "        but for k and v it should find the affinity to backward and forward window\n",
    "        then the net length of each window should incvrease from window_size to \n",
    "        window_size*(backward+forward+1=3)\n",
    "        the again make it for each window the context length to 3*window_size \n",
    "        for such num_windows \n",
    "        \n",
    "        '''\n",
    "        return gathered \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, attention_mask = None):\n",
    "        b, ori_seq_len, d = x.shape \n",
    "        h = self.num_heads \n",
    "        head_dim = self.head_dim \n",
    "\n",
    "        q = self.q_proj(x).view(b, ori_seq_len, h, head_dim).permute(0, 2, 1, 3).contiguous()\n",
    "        k = self.k_proj(x).view(b, ori_seq_len, h, head_dim).permute(0,2,1,3).contiguous()\n",
    "        v = self.k_proj(x).view(b, ori_seq_len, h, head_dim).permute(0,2,1,3).contiguous()\n",
    "\n",
    "        ### Merge together Head/Batch Dimension ###\n",
    "        q = q.flatten(0,1) #batched_head(b*h), seq_len, head_dim\n",
    "        k = k.flatten(0,1)\n",
    "        v = v.flatten(0,1)\n",
    "        device = q.device\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.repeat(self.num_heads, 1) \n",
    "\n",
    "        if ori_seq_len % self.window_size == 0:\n",
    "            difference = self.window_size*math.ceil(ori_seq_len/self.window_size) - ori_seq_len \n",
    "            q = nn.functional.pad(q, pad=(0,0,0, difference))\n",
    "            k = nn.functional.pad(k, pad=(0,0,0, difference))\n",
    "            v = nn.functional.pad(v, pad=(0,0,0, difference))\n",
    "\n",
    "        seq_len = q.shape[1]\n",
    "        num_windows = seq_len // self.window_size \n",
    "        idx = torch.arange(seq_len, device=device)\n",
    "        bucketed_idx = idx.reshape(1, num_windows, self.window_size) #bs,n, win_size of indexes\n",
    "\n",
    "\n",
    "        ### Bucket our Q,K,V into the Chunked Windows ###\n",
    "        bucketed_q = q.reshape(b*self.num_heads, num_windows, self.window_size, head_dim)\n",
    "        bucketed_k = k.reshape(b*self.num_heads, num_windows, self.window_size, head_dim)\n",
    "        bucketed_v = v.reshape(b*self.num_heads, num_windows, self.window_size, head_dim)\n",
    "\n",
    "\n",
    "        #b*h, num_windows, 3*win_size, embed_dim\n",
    "        bucketed_k = self.collect_windows(bucketed_k, self.look_backward, self.look_forward)\n",
    "        bucketed_v = self.collect_windows(bucketed_v, self.look_backward, self.look_forward)\n",
    "        # to know the pad token lets collect through collect window \n",
    "        # batchsize*heads, num_win, win_size\n",
    "        collected_bucket_idx = self.collect_windows(bucketed_idx,self.look_backward, self.look_forward)\n",
    "        bucket_pad_mask = (collected_bucket_idx == -1) #b, num_win, 3*win_size \n",
    "\n",
    "\n",
    "        attention_scores = bucketed_q @ bucketed_k.transpose(-1, -2) \n",
    "        # b*h, num_win, win_size, embed_dim @ b*h, num_win, 3*win_size, embed_dim\n",
    "        # attention_score has dim b*h, num_win, win_size(query tokens list length), 3*win_size(key tokens length list) \n",
    "        \n",
    "        # b=1, num_win, win_size=512, 3*win_size=1536 same as attetion scores\n",
    "        bucket_pad_mask = bucket_pad_mask.unsqueeze(-2).repeat(1,1,self.window_size, 1)\n",
    "        ''' \n",
    "        bucket_pad_mask originally told us:\n",
    "        In this window, which key tokens are fake pads?\n",
    "        But attention is computed query Ã— key.\n",
    "        So we need:\n",
    "        For every query token in this window, which key tokens are fake pads?\n",
    "        \n",
    "        '''\n",
    "        attention_scores = attention_scores.masked_fill(bucket_pad_mask, float(\"-inf\"))\n",
    "        collected_bucket_idx = collected_bucket_idx.unsqueeze(-2).repeat(1,1,self.window_size, 1)\n",
    "\n",
    "        # causal masking \n",
    "\n",
    "\n",
    "\n",
    "        #non causal masking\n",
    "        if not self.causal:\n",
    "            num_concat_windows = (self.look_backward + self.look_forward + 1)\n",
    "            repeated_query_idx = idx.reshape(1, -1, self.window_size, 1).repeat(1,1,1,self.window_size*num_concat_windows)\n",
    "\n",
    "            total_look_backward = (self.window_size*self.look_backward)\n",
    "            total_look_forward = (self.window_size*self.look_backward)\n",
    "\n",
    "            max_idx = repeated_query_idx + total_look_forward \n",
    "            min_idx = repeated_query_idx - total_look_backward \n",
    "\n",
    "            upper_mask = ((collected_bucket_idx>max_idx) & (collected_bucket_idx !=-1))\n",
    "            lower_mask = ((collected_bucket_idx<min_idx) & (collected_bucket_idx != -1))\n",
    "\n",
    "            overcompute_mask = upper_mask | lower_mask \n",
    "\n",
    "            causal_mask = torch.zeros_like(attention_scores, device=device).bool() \n",
    "\n",
    "        else:\n",
    "            num_concat_windows = (self.look_backward + self.look_backward + 1)\n",
    "            repeated_query_idx = idx.reshape(1,-1,self.window_size,1).repeat(1,1,1,self.window_size*num_concat_windows)\n",
    "\n",
    "            causal_mask = (collected_bucket_idx>repeated_query_idx)\n",
    "            total_look_backward = (self.window_size*self.look_backward)\n",
    "            min_idx = repeated_query_idx - total_look_backward \n",
    "            overcompute_mask = ((collected_bucket_idx<min_idx) & (collected_bucket_idx != -1))\n",
    "\n",
    "        if attention_mask is not None:\n",
    "\n",
    "            ''' \n",
    "            this attention mask is for making the sequence_len uniform from \n",
    "            data collator and dataloader while data preparation for input \n",
    "            after tokenization step \n",
    "            we dont want query to attend those tokens so we mask out them and \n",
    "            dont involve them in attention calculation\n",
    "            this is done with padding \n",
    "\n",
    "            apart form another padding being to make the sequence_len % num_windows == 0 \n",
    "            '''\n",
    "\n",
    "            if ori_seq_len % self.window_size !=0 :\n",
    "                diff = self.window_size * math.ceil(ori_seq_len/self.window_size) - ori_seq_len\n",
    "                attention_mask = nn.functional.pad(attention_mask, pad=(0, diff), value=-1)\n",
    "\n",
    "            #chunk into buckets \n",
    "            # b*h, 4, 3\n",
    "            attention_mask = attention_mask.reshape(b*h, num_windows, self.window_size)\n",
    "            #includes mask to neighbours (3*win_size)\n",
    "            #b*h, num_windows, 3*win_size\n",
    "            attention_mask = self.collect_windows(attention_mask, self.look_backward, self.look_forward)\n",
    "            #b*h, num_windows, 3*win_size -> b*h, num_windows, win_size, 3*win_Size\n",
    "            ''' \n",
    "            Before: (batch_heads, num_windows, key_len)\n",
    "            After : (batch_heads, num_windows, query_len, key_len)\n",
    "            '''\n",
    "\n",
    "            attention_mask = attention_mask.unsqueeze(-2).repeat(1,1,self.window_size,1)\n",
    "            attention_mask = (attention_mask==0) \n",
    "\n",
    "        else:\n",
    "            # we want every query token to attend the other token no padded token in the sequence\n",
    "            # 1 means masked out 0  means dont maks out that token or index \n",
    "            attention_mask = torch.zeros_like(attention_scores, device=device).bool()\n",
    "\n",
    "\n",
    "        combined_mask = overcompute_mask | attention_mask \n",
    "        if self.causal:\n",
    "            combined_mask = combined_mask | causal_mask \n",
    "\n",
    "        #replace the masked value with -inf for softmax\n",
    "        mask_value = float(\"-inf\")\n",
    "        attention_scores = attention_scores.masked_fill(combined_mask, mask_value)\n",
    "\n",
    "        #softmax \n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores) \n",
    "        #b*h, num_windows, query_len(win_size), key_len (3*win_size)\n",
    "\n",
    "        # attn @ v\n",
    "        #b*h, num_win, win_size, 3*win_size @ b*h, num_win, 3*win_size, head_dim\n",
    "        # output b*h, num_win, win_size, head_dim\n",
    "        output = attention_scores @ bucketed_v \n",
    "        output = output.reshape(b, h, -1, head_dim) #b, h, num_win*win_size, head_dim\n",
    "        output = output[:, :, :ori_seq_len].permute(0,2,1,3).flatten(2)  #b, orig_seq_len, embed_dim \n",
    "        output = self.out_proj(output)\n",
    "\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        q = self.split_into_windows(q) #b, h, num_w, win_size,head_dim\n",
    "        k = self.split_into_windows(k)\n",
    "        v = self.split_into_windows(v) \n",
    "\n",
    "        #attention in each windows \n",
    "        scores = q @ k.transpose(-1, -2) #b, h, num_w, win_size, win_size \n",
    "        scores = scores/math.sqrt(head_dim) \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = attn @ v #b, h, num_w, win_size, head_dim \n",
    "\n",
    "        out = out.reshape(b, h, seq_len, head_dim).permute(0,2,1,3).reshape(b, seq_len, d)\n",
    "        return scores, self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f15c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "# attn = LocalWindowAttention()\n",
    "# scores, out = attn(x) \n",
    "# print(\"Input:\", x.shape)\n",
    "# print(\"Output:\", out.shape)\n",
    "# print(scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    rand = torch.randn(4,256,768)\n",
    "    attention = LocalWindowAttention(window_size=64)\n",
    "    out = attention(rand)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f57ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d1697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9687ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c663cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280396a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2807c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc2a458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5a96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965acb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71a814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7986ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd3579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccddd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28046437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9443955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24241c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
