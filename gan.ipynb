{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "5nP_WUZ9FN-R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.tensorboard import SummaryWriter  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "HOGoxeeJFN-T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "iiWhk4i2QhCr"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    latent_vector_size = 100\n",
        "    device = 'cpu'\n",
        "    batch_size = 128\n",
        "    initial_lr = 0.1\n",
        "    final_lr =1e-6\n",
        "    decay_factor = 1.00004\n",
        "    momentum_initial = 0.5\n",
        "    final_momentum_value = 0.7\n",
        "    dropout = 0.5\n",
        "    num_classes = 10\n",
        "    img_size = 64\n",
        "    no_of_lables = 10\n",
        "    no_of_channels = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "CQ_fOrMNQhCr"
      },
      "outputs": [],
      "source": [
        "ModelArgs.device = device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Transforms for images\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    transforms.Resize(size=(ModelArgs.img_size,ModelArgs.img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjduuQ6CFN-T",
        "outputId": "c6fa2917-3f61-4fd8-8295-3a142d01cce4"
      },
      "outputs": [],
      "source": [
        "#Loading MNIST Dataset\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "data_path = Path('/home/cmi_10101/Documents/datasets/content/data/')\n",
        "\n",
        "# train_dir = data_path / \"train\"\n",
        "# test_dir = data_path / \"test\"\n",
        "\n",
        "# Load the training set\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=ModelArgs.batch_size, shuffle=True)\n",
        "\n",
        "# Load the test set\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=ModelArgs.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "Mf6_wWp-FN-T"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)  #mean = 0, std = 0.02\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ModelArgs.img_size = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "igQOSe-jFN-U"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_vector_size = 100,\n",
        "        no_of_channels = 1,\n",
        "        kernel_size = (4,4),\n",
        "        stride: int = 2,\n",
        "        number_of_feature_maps: int = 64,\n",
        "        padding: int = 1,\n",
        "        z_out_dimensions: int = 200,\n",
        "        y_out_dimensions: int = 1000,\n",
        "        labels_size: int = 10,\n",
        "        combined_hidden_layer_dimensions: int = 1200,\n",
        "        img_size: int = 1\n",
        "\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # self.linear_z_out = nn.Linear(in_features=latent_vector_size, out_features=z_out_dimensions, device=ModelArgs.device)\n",
        "        # self.linear_y_out = nn.Linear(in_features=ModelArgs.no_of_lables, out_features=y_out_dimensions, device=ModelArgs.device)\n",
        "        # self.combined = z_out_dimensions + y_out_dimensions\n",
        "        # self.combined_layer = nn.Linear(in_features=self.combined, out_features=combined_hidden_layer_dimensions, device=ModelArgs.device)\n",
        "        self.dense = nn.Linear(in_features=latent_vector_size, out_features=img_size * img_size, device=ModelArgs.device)\n",
        "        self.combined_hidden_layer_dimensions = latent_vector_size + ModelArgs.no_of_lables\n",
        "        self.embedding = nn.Embedding(num_embeddings=ModelArgs.num_classes, embedding_dim=latent_vector_size, device=ModelArgs.device)\n",
        "        # self.wi = weight_initialization()\n",
        "        self.img_size = img_size\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "\n",
        "            # nn.Linear(in_features=latent_vector_size, out_features=z_out_dimensions, device=ModelArgs.device),\n",
        "            # nn.Linear(in_features=labels_size, out_features=y_out_dimensions, device=ModelArgs.device),\n",
        "            nn.ConvTranspose2d(ModelArgs.latent_vector_size + ModelArgs.latent_vector_size, number_of_feature_maps * 16 , kernel_size=kernel_size, stride=stride, padding=0, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #shape = (...,1024, 4, 4)\n",
        "            nn.ConvTranspose2d(number_of_feature_maps * 16, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            #shape = (..., 512, 8, 8)\n",
        "            nn.ConvTranspose2d(number_of_feature_maps * 8, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 4),\n",
        "\n",
        "             #shape = (..., 256, 16, 16)\n",
        "            nn.ConvTranspose2d(number_of_feature_maps * 4, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 2),\n",
        "\n",
        "             #shape = (..., 128, 32, 32)\n",
        "            nn.ConvTranspose2d(number_of_feature_maps * 2, no_of_channels , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.Tanh()\n",
        "            #shape = (..., 3, 64, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        labels = self.embedding(y)\n",
        "\n",
        "        labels = labels.unsqueeze(2).unsqueeze(3).view(x.shape[0], ModelArgs.latent_vector_size, 1,1)\n",
        "\n",
        "        combined = torch.cat([x, labels], dim=1)\n",
        "        # print(combined.shape)\n",
        "        out = self.main(combined)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-xlwOgCFN-U",
        "outputId": "22b21844-9096-45bd-f76a-ffc8dc9dd253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (dense): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (embedding): Embedding(10, 100)\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(200, 1024, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): ReLU()\n",
            "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): ReLU()\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Intializing the Generator instance\n",
        "generator = Generator().to(ModelArgs.device)\n",
        "\n",
        "#Applying the weights transformation\n",
        "generator.apply(weights_init)\n",
        "\n",
        "#Printing the structure\n",
        "print(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlZVuv-oQhCt",
        "outputId": "28ba05d5-6e4a-40ab-8de2-fc5014eae39d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 1])"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randint(0, 10, (128, 1), dtype=torch.long, device=ModelArgs.device).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x5GjkITFN-U",
        "outputId": "831c01f2-0e4d-489e-de78-298656893b67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "Generator (Generator)                    [128, 100, 1, 1]     [128, 1, 64, 64]     101                  True\n",
              "├─Embedding (embedding)                  [128]                [128, 100]           1,000                True\n",
              "├─Sequential (main)                      [128, 200, 1, 1]     [128, 1, 64, 64]     --                   True\n",
              "│    └─ConvTranspose2d (0)               [128, 200, 1, 1]     [128, 1024, 4, 4]    3,276,800            True\n",
              "│    └─BatchNorm2d (1)                   [128, 1024, 4, 4]    [128, 1024, 4, 4]    2,048                True\n",
              "│    └─ReLU (2)                          [128, 1024, 4, 4]    [128, 1024, 4, 4]    --                   --\n",
              "│    └─ConvTranspose2d (3)               [128, 1024, 4, 4]    [128, 512, 8, 8]     8,388,608            True\n",
              "│    └─BatchNorm2d (4)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
              "│    └─ReLU (5)                          [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
              "│    └─ConvTranspose2d (6)               [128, 512, 8, 8]     [128, 256, 16, 16]   2,097,152            True\n",
              "│    └─ReLU (7)                          [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
              "│    └─BatchNorm2d (8)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
              "│    └─ConvTranspose2d (9)               [128, 256, 16, 16]   [128, 128, 32, 32]   524,288              True\n",
              "│    └─ReLU (10)                         [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
              "│    └─BatchNorm2d (11)                  [128, 128, 32, 32]   [128, 128, 32, 32]   256                  True\n",
              "│    └─ConvTranspose2d (12)              [128, 128, 32, 32]   [128, 1, 64, 64]     2,048                True\n",
              "│    └─Tanh (13)                         [128, 1, 64, 64]     [128, 1, 64, 64]     --                   --\n",
              "========================================================================================================================\n",
              "Total params: 14,293,837\n",
              "Trainable params: 14,293,837\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 213.94\n",
              "========================================================================================================================\n",
              "Input size (MB): 0.05\n",
              "Forward/backward pass size (MB): 507.61\n",
              "Params size (MB): 57.17\n",
              "Estimated Total Size (MB): 564.84\n",
              "========================================================================================================================"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "random_data = torch.randn(ModelArgs.batch_size, ModelArgs.latent_vector_size, 1, 1, device=ModelArgs.device)\n",
        "# labels =\n",
        "labels = torch.randint(0, 10, (128,), dtype=torch.long, device=ModelArgs.device)\n",
        "random_data = random_data.to(ModelArgs.device)\n",
        "summary(model=generator,\n",
        "        \n",
        "        # input_size=(128, 100, 10, 1, 1),\n",
        "        input_data=(random_data, labels),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRkap4OeO-qS",
        "outputId": "c8104443-2f82-4357-f65d-cd87d00a5570"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 100]), torch.Size([128]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_data.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "OLAsBWTOQhCt"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(labels, num_classes=10):\n",
        "    # Create a tensor of zeros with shape (number_of_labels, num_classes)\n",
        "    one_hot = torch.zeros(labels.size(0), num_classes, device=labels.device)\n",
        "\n",
        "    # Scatter the labels to create one-hot encoding\n",
        "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "for images, labels in trainloader:\n",
        "    # Convert labels to one-hot encoded vectors\n",
        "    # print(images)\n",
        "    # print(labels)\n",
        "    one_hot_labels = one_hot_encode(labels)\n",
        "    break\n",
        "\n",
        "    # Now you can use `one_hot_labels` as needed\n",
        "    # print(one_hot_labels.shape)\n",
        "    # break  # Break after the first batch for demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "izJWPwLkFN-U"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        no_of_channels = 1,\n",
        "        kernel_size = (4,4),\n",
        "        stride: int = 2,\n",
        "        number_of_feature_maps: int = 64,\n",
        "        padding: int = 1,\n",
        "        lr_slope=0.2,\n",
        "        num_classes: int = ModelArgs.num_classes,\n",
        "        z_out_dimensions: int = 200,\n",
        "        y_out_dimensions: int = 1000,\n",
        "        labels_size: int = 10,\n",
        "        combined_hidden_layer_dimensions: int = 1200,\n",
        "        latent_vector_size: int = ModelArgs.latent_vector_size,\n",
        "        img_size: int = ModelArgs.img_size\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=ModelArgs.num_classes, embedding_dim=ModelArgs.img_size * ModelArgs.img_size, device=ModelArgs.device)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(no_of_channels + 1, number_of_feature_maps * 2 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 2),\n",
        "            nn.LeakyReLU(negative_slope=lr_slope),\n",
        "\n",
        "                #shape = (...,1024, 32, 32)\n",
        "            nn.Conv2d(number_of_feature_maps * 2, number_of_feature_maps * 4 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 4),\n",
        "            nn.LeakyReLU(negative_slope=lr_slope),\n",
        "\n",
        "                #shape = (..., 512, 16, 16)\n",
        "            nn.Conv2d(number_of_feature_maps * 4, number_of_feature_maps * 8 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 8),\n",
        "            nn.LeakyReLU(negative_slope=lr_slope),\n",
        "\n",
        "                #shape = (..., 256, 8, 8)\n",
        "            nn.Conv2d(number_of_feature_maps * 8, number_of_feature_maps * 16 , kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.InstanceNorm2d(number_of_feature_maps * 16),\n",
        "            nn.LeakyReLU(negative_slope=lr_slope),\n",
        "            #  shape = (..., 128, 4, 4)\n",
        "\n",
        "            nn.Conv2d(number_of_feature_maps * 16, 1 , kernel_size=kernel_size, stride=4, padding=padding, bias=False),\n",
        "            #shape = (...,1,1)\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid(),\n",
        "         )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # (2800x28 and 200x100)\n",
        "        y = self.embedding(y)\n",
        "        B,E = y.shape\n",
        "\n",
        "        combined = torch.concat([x, y.unsqueeze(2).unsqueeze(3).view(x.shape[0], ModelArgs.no_of_channels, ModelArgs.img_size, ModelArgs.img_size)], dim=1)\n",
        "\n",
        "        x = self.main(combined)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x.view(B, C, IMG_SIZE*IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2AMYty_pQhCt",
        "outputId": "000a2e4b-33e9-441e-9226-1818a29b0eb2"
      },
      "outputs": [],
      "source": [
        "# ModelArgs.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUQEbynaFN-U",
        "outputId": "9adcde31-f2e4-41b5-aa50-4bbbab863515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (linear_z_out): Linear(in_features=1, out_features=200, bias=True)\n",
            "  (final_linear_layer): Linear(in_features=1200, out_features=1, bias=True)\n",
            "  (linear_y_out): Linear(in_features=10, out_features=1000, bias=True)\n",
            "  (combined_layer): Linear(in_features=1200, out_features=1200, bias=True)\n",
            "  (embedding): Embedding(10, 4096)\n",
            "  (sig): Sigmoid()\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(2, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "    (3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): LeakyReLU(negative_slope=0.2)\n",
            "    (9): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): LeakyReLU(negative_slope=0.2)\n",
            "    (12): Conv2d(1024, 1, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False)\n",
            "    (13): Flatten(start_dim=1, end_dim=-1)\n",
            "    (14): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#Intializing the Discriminator instance\n",
        "discriminator = Discriminator().to(ModelArgs.device)\n",
        "#Apply the wieght intilization function layer by layer\n",
        "discriminator = discriminator.apply(weights_init)\n",
        "#Printing the structure\n",
        "print(discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nswZOUxFN-V",
        "outputId": "d6362f39-7a1d-4a23-8d8a-ae8d5c264672"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "========================================================================================================================\n",
              "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
              "========================================================================================================================\n",
              "Discriminator (Discriminator)            [128, 1, 64, 64]     [128, 1]             1,453,801            True\n",
              "├─Embedding (embedding)                  [128]                [128, 4096]          40,960               True\n",
              "├─Sequential (main)                      [128, 2, 64, 64]     [128, 1]             --                   True\n",
              "│    └─Conv2d (0)                        [128, 2, 64, 64]     [128, 128, 32, 32]   4,096                True\n",
              "│    └─BatchNorm2d (1)                   [128, 128, 32, 32]   [128, 128, 32, 32]   256                  True\n",
              "│    └─LeakyReLU (2)                     [128, 128, 32, 32]   [128, 128, 32, 32]   --                   --\n",
              "│    └─Conv2d (3)                        [128, 128, 32, 32]   [128, 256, 16, 16]   524,288              True\n",
              "│    └─BatchNorm2d (4)                   [128, 256, 16, 16]   [128, 256, 16, 16]   512                  True\n",
              "│    └─LeakyReLU (5)                     [128, 256, 16, 16]   [128, 256, 16, 16]   --                   --\n",
              "│    └─Conv2d (6)                        [128, 256, 16, 16]   [128, 512, 8, 8]     2,097,152            True\n",
              "│    └─BatchNorm2d (7)                   [128, 512, 8, 8]     [128, 512, 8, 8]     1,024                True\n",
              "│    └─LeakyReLU (8)                     [128, 512, 8, 8]     [128, 512, 8, 8]     --                   --\n",
              "│    └─Conv2d (9)                        [128, 512, 8, 8]     [128, 1024, 4, 4]    8,388,608            True\n",
              "│    └─BatchNorm2d (10)                  [128, 1024, 4, 4]    [128, 1024, 4, 4]    2,048                True\n",
              "│    └─LeakyReLU (11)                    [128, 1024, 4, 4]    [128, 1024, 4, 4]    --                   --\n",
              "│    └─Conv2d (12)                       [128, 1024, 4, 4]    [128, 1, 1, 1]       16,384               True\n",
              "│    └─Flatten (13)                      [128, 1, 1, 1]       [128, 1]             --                   --\n",
              "│    └─Sigmoid (14)                      [128, 1]             [128, 1]             --                   --\n",
              "========================================================================================================================\n",
              "Total params: 12,529,129\n",
              "Trainable params: 12,529,129\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 52.08\n",
              "========================================================================================================================\n",
              "Input size (MB): 2.10\n",
              "Forward/backward pass size (MB): 507.51\n",
              "Params size (MB): 44.30\n",
              "Estimated Total Size (MB): 553.91\n",
              "========================================================================================================================"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "images = torch.randn(128, 1, 64, 64)\n",
        "labels = torch.randint(0, 10, (128,), dtype=torch.long)\n",
        "\n",
        "summary(model=discriminator,\n",
        "        # input_size=(100, 1, 64, 64, 10),\n",
        "        input_data=(images.to(ModelArgs.device), labels.to(ModelArgs.device)),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HADuDK9Ac_88",
        "outputId": "5e1bb2f4-81bb-4610-ccbb-1aa23228f093"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 1, 64, 64]), torch.Size([128]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wS1rzq2jnhP9"
      },
      "outputs": [],
      "source": [
        "# labels = labels.float()\n",
        "labels = one_hot_encode(labels)\n",
        "labels = labels.to(ModelArgs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRiopT6rnniq",
        "outputId": "9d07a838-7987-4f6d-b0cf-d4f5bcbe7d12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
              "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "ugXl-J8cFN-V"
      },
      "outputs": [],
      "source": [
        "# epochs = 5 #30\n",
        "# # beta_1 = 0.5\n",
        "# # lr_optimizer = 0.0002\n",
        "# loss_fn = nn.BCELoss()  #BCELoss function\n",
        "\n",
        "generator = Generator().to(ModelArgs.device).apply(weights_init)\n",
        "discriminator = Discriminator().to(ModelArgs.device).apply(weights_init)\n",
        "\n",
        "# optimizerD = torch.optim.SGD(params=discriminator.parameters(), momentum=ModelArgs.momentum_initial, lr=ModelArgs.initial_lr) #For discriminator\n",
        "# optimizerG = torch.optim.SGD(params=generator.parameters(), momentum=ModelArgs.momentum_initial, lr=ModelArgs.initial_lr) #For generator\n",
        "\n",
        "\n",
        "epochs = 30 #30\n",
        "beta_1 = 0.5\n",
        "lr_optimizer = 0.0002\n",
        "loss_fn = nn.BCELoss()  #BCELoss function\n",
        "\n",
        "\n",
        "optimizerD = torch.optim.Adam(params=discriminator.parameters(), betas=(beta_1, 0.999), lr=lr_optimizer) #For discriminator\n",
        "optimizerG = torch.optim.Adam(params=generator.parameters(), betas=(beta_1, 0.999), lr=lr_optimizer) #For generator\n",
        "\n",
        "\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "\n",
        "loss_g = []\n",
        "loss_d = []\n",
        "img_list = []\n",
        "\n",
        "# Fixed noise for generating the images\n",
        "fixed_noise = torch.randn((ModelArgs.batch_size, ModelArgs.latent_vector_size, 1, 1), dtype=torch.float32, device=ModelArgs.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "fJps83qfFN-V"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "save_images = Path('output_images/MNIST')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwNpigl9UxjS",
        "outputId": "52eba7f5-ae64-43d0-ef5b-a7fa8790ed04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 64, 64])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "for images, labels in trainloader:\n",
        "  print(images.shape)\n",
        "  print(labels.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_hot_encode(torch.randint(low=0, high=ModelArgs.num_classes, size=(current_batch_size,), device=ModelArgs.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "05j9aKXJFN-V",
        "outputId": "65200ebf-480f-4d01-d8f2-c98ee184cec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iterations:  0 Epoch:  0 Generator loss:  3.579348564147949 Discriminator loss:  2.062045097351074\n",
            "saving the output\n",
            "Iterations:  300 Epoch:  0 Generator loss:  6.042276382446289 Discriminator loss:  0.8542827367782593\n",
            "saving the output\n",
            "Iterations:  600 Epoch:  1 Generator loss:  0.7866476774215698 Discriminator loss:  0.5937706828117371\n",
            "Iterations:  900 Epoch:  1 Generator loss:  5.631103515625 Discriminator loss:  0.006849944591522217\n",
            "saving the output\n",
            "Iterations:  1200 Epoch:  2 Generator loss:  6.262673854827881 Discriminator loss:  0.025467010214924812\n",
            "Iterations:  1500 Epoch:  3 Generator loss:  4.356583595275879 Discriminator loss:  0.05104273557662964\n",
            "saving the output\n",
            "Iterations:  1800 Epoch:  3 Generator loss:  8.941598892211914 Discriminator loss:  0.0033288539852946997\n",
            "saving the output\n",
            "Iterations:  2100 Epoch:  4 Generator loss:  7.465685844421387 Discriminator loss:  0.0012019629357382655\n",
            "Iterations:  2400 Epoch:  5 Generator loss:  7.324221611022949 Discriminator loss:  0.0025390232913196087\n",
            "saving the output\n",
            "Iterations:  2700 Epoch:  5 Generator loss:  3.095341682434082 Discriminator loss:  1.3576617240905762\n",
            "Iterations:  3000 Epoch:  6 Generator loss:  6.506522178649902 Discriminator loss:  1.089545726776123\n",
            "saving the output\n",
            "Iterations:  3300 Epoch:  7 Generator loss:  2.3093085289001465 Discriminator loss:  0.48193496465682983\n",
            "saving the output\n",
            "Iterations:  3600 Epoch:  7 Generator loss:  3.468996047973633 Discriminator loss:  0.19674235582351685\n",
            "Iterations:  3900 Epoch:  8 Generator loss:  4.445418834686279 Discriminator loss:  0.20407183468341827\n",
            "saving the output\n",
            "Iterations:  4200 Epoch:  8 Generator loss:  5.091259956359863 Discriminator loss:  0.16681990027427673\n",
            "Iterations:  4500 Epoch:  9 Generator loss:  4.631828784942627 Discriminator loss:  0.11116449534893036\n",
            "saving the output\n",
            "Iterations:  4800 Epoch:  10 Generator loss:  5.054797172546387 Discriminator loss:  0.06426990777254105\n",
            "saving the output\n",
            "Iterations:  5100 Epoch:  10 Generator loss:  4.175775527954102 Discriminator loss:  0.02118494175374508\n",
            "Iterations:  5400 Epoch:  11 Generator loss:  3.6372594833374023 Discriminator loss:  0.0510992631316185\n",
            "saving the output\n",
            "Iterations:  5700 Epoch:  12 Generator loss:  3.334332227706909 Discriminator loss:  0.01614893414080143\n",
            "Iterations:  6000 Epoch:  12 Generator loss:  3.485320806503296 Discriminator loss:  0.01915552094578743\n",
            "saving the output\n",
            "Iterations:  6300 Epoch:  13 Generator loss:  4.881424903869629 Discriminator loss:  0.1340731680393219\n",
            "saving the output\n",
            "Iterations:  6600 Epoch:  14 Generator loss:  4.5649824142456055 Discriminator loss:  0.059026479721069336\n",
            "Iterations:  6900 Epoch:  14 Generator loss:  4.658280849456787 Discriminator loss:  0.019900253042578697\n",
            "saving the output\n",
            "Iterations:  7200 Epoch:  15 Generator loss:  5.072626113891602 Discriminator loss:  0.03824182227253914\n",
            "Iterations:  7500 Epoch:  15 Generator loss:  7.448937892913818 Discriminator loss:  0.1585247665643692\n",
            "saving the output\n",
            "Iterations:  7800 Epoch:  16 Generator loss:  7.189703464508057 Discriminator loss:  0.031238606199622154\n",
            "saving the output\n",
            "Iterations:  8100 Epoch:  17 Generator loss:  4.037770748138428 Discriminator loss:  0.04680778831243515\n",
            "Iterations:  8400 Epoch:  17 Generator loss:  5.023361682891846 Discriminator loss:  0.0175824835896492\n",
            "saving the output\n",
            "Iterations:  8700 Epoch:  18 Generator loss:  7.642618179321289 Discriminator loss:  0.013222966343164444\n",
            "Iterations:  9000 Epoch:  19 Generator loss:  5.00094747543335 Discriminator loss:  0.061222512274980545\n",
            "saving the output\n",
            "Iterations:  9300 Epoch:  19 Generator loss:  5.67946720123291 Discriminator loss:  0.03315814584493637\n",
            "saving the output\n",
            "Iterations:  9600 Epoch:  20 Generator loss:  4.767733097076416 Discriminator loss:  0.0843457281589508\n",
            "Iterations:  9900 Epoch:  21 Generator loss:  6.149924278259277 Discriminator loss:  0.022474493831396103\n",
            "saving the output\n",
            "Iterations:  10200 Epoch:  21 Generator loss:  4.6455397605896 Discriminator loss:  0.012049252167344093\n",
            "Iterations:  10500 Epoch:  22 Generator loss:  5.443190097808838 Discriminator loss:  0.007799830753356218\n",
            "saving the output\n",
            "Iterations:  10800 Epoch:  23 Generator loss:  6.868170261383057 Discriminator loss:  0.009508968330919743\n",
            "saving the output\n",
            "Iterations:  11100 Epoch:  23 Generator loss:  5.594584941864014 Discriminator loss:  0.0036391238681972027\n",
            "Iterations:  11400 Epoch:  24 Generator loss:  7.631223678588867 Discriminator loss:  0.08822087198495865\n",
            "saving the output\n",
            "Iterations:  11700 Epoch:  24 Generator loss:  4.359555721282959 Discriminator loss:  0.04220426082611084\n",
            "Iterations:  12000 Epoch:  25 Generator loss:  5.270406246185303 Discriminator loss:  0.038045261055231094\n",
            "saving the output\n",
            "Iterations:  12300 Epoch:  26 Generator loss:  5.727649688720703 Discriminator loss:  0.02262416109442711\n",
            "saving the output\n",
            "Iterations:  12600 Epoch:  26 Generator loss:  5.962903022766113 Discriminator loss:  0.026564985513687134\n",
            "Iterations:  12900 Epoch:  27 Generator loss:  5.9047393798828125 Discriminator loss:  0.024372929707169533\n",
            "saving the output\n",
            "Iterations:  13200 Epoch:  28 Generator loss:  5.354524612426758 Discriminator loss:  0.008409504778683186\n",
            "Iterations:  13500 Epoch:  28 Generator loss:  4.835018157958984 Discriminator loss:  0.019354496151208878\n",
            "saving the output\n",
            "Iterations:  13800 Epoch:  29 Generator loss:  6.298248291015625 Discriminator loss:  0.033287372440099716\n",
            "saving the output\n"
          ]
        }
      ],
      "source": [
        "#Training loop\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "iters = 0\n",
        "\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "writer_real = SummaryWriter(f\"logs/real\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for X, y in trainloader:\n",
        "\n",
        "        X = X.to(ModelArgs.device)\n",
        "        y = y.to(ModelArgs.device)\n",
        "        #Train the discriminator (with real data)\n",
        "\n",
        "        ############################\n",
        "        # (1) Update D network: maximize: log(1 - D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        current_batch_size = X.shape[0]  #Getting the current batch size\n",
        "        \n",
        "        real_data = torch.ones((current_batch_size,), device=device, dtype=torch.float32)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = discriminator(X, y).view(-1)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss_real = loss_fn(y_pred, real_data)\n",
        "\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizerD.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss_real.backward(retain_graph=True)\n",
        "\n",
        "\n",
        "        #Train the discriminator (with fake data)\n",
        "\n",
        "        noise = torch.randn((current_batch_size, ModelArgs.latent_vector_size, 1, 1), device=device)\n",
        "        fake_data = torch.zeros((current_batch_size,), device=device, dtype=torch.float32)\n",
        "        fake_label = torch.randint(0, ModelArgs.no_of_lables, (current_batch_size, ), device=ModelArgs.device)\n",
        "        noise_generated_by_generator = generator(noise, fake_label)\n",
        "\n",
        "        #1. Forward pass\n",
        "        y_pred = discriminator(noise_generated_by_generator, fake_label).view(-1)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss_fake = loss_fn(y_pred, fake_data)\n",
        "\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss_fake.backward(retain_graph=True)\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizerD.step()\n",
        "\n",
        "        #Accumulating total discriminator loss\n",
        "        discriminator_combined_loss = loss_real + loss_fake\n",
        "        loss_d.append(discriminator_combined_loss.item())\n",
        "\n",
        "\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "\n",
        "        labels = torch.ones((current_batch_size,), device=device, dtype=torch.float32)\n",
        "\n",
        "        #1. Forward pass\n",
        "        noise_generated_by_generator = generator(noise, y)\n",
        "        y_pred = discriminator(noise_generated_by_generator, y).view(-1)\n",
        "        # y_pred = torch.argmax(probs, dim=1).type(torch.float32)\n",
        "\n",
        "\n",
        "        #2. Calculate and accumulate loss\n",
        "        loss_gen = loss_fn(y_pred, labels)\n",
        "\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizerG.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss_gen.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizerG.step()\n",
        "\n",
        "        loss_g.append(loss_gen.item())\n",
        "\n",
        "        if iters % 300 == 0:\n",
        "          print(\"Iterations: \", iters, \"Epoch: \", epoch, \"Generator loss: \", loss_gen.item(), \"Discriminator loss: \", discriminator_combined_loss.item())\n",
        "\n",
        "        #save the output\n",
        "        with torch.no_grad():\n",
        "          if iters % 500 == 0:\n",
        "            print('saving the output')\n",
        "            torchvision.utils.save_image(X,'{}/real_images_steps_{}.png'.format(save_images, iters),normalize=True)\n",
        "            fake = generator(fixed_noise, y)\n",
        "            torchvision.utils.save_image(fake,'{}/fake_images_steps_{}.png'.format(save_images, iters),normalize=True)\n",
        "\n",
        "            img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "            img_grid_real = torchvision.utils.make_grid(X, normalize=True)\n",
        "                \n",
        "            writer_fake.add_image(\n",
        "                      \"Mnist Fake Images\", img_grid_fake, global_step=iters\n",
        "                  )\n",
        "            writer_real.add_image(\n",
        "                      \"Mnist Real Images\", img_grid_real, global_step=iters\n",
        "                  )\n",
        "                  \n",
        "\n",
        "            # Check pointing for every epoch\n",
        "            # torch.save(generator.state_dict(), 'weights/CelebA/generator_steps_%d.pth' % (iters))\n",
        "            # torch.save(discriminator.state_dict(), 'weights/CelebA/discriminator_steps_%d.pth' % (iters))\n",
        "\n",
        "\n",
        "          iters += 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
