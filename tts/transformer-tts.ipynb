{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9wM8HrXVx5A",
        "outputId": "581b4e4e-978e-43e1-aabf-c7a8d17a0855"
      },
      "outputs": [],
      "source": [
        "#Transformer version of TacoTron\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "e4b2df23-1b2f-4ca2-9968-9bbd2972779d",
        "_uuid": "4b54e35b-02bb-44b4-adca-a297c3081d36",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:27:50.949322Z",
          "iopub.status.busy": "2025-03-16T20:27:50.949124Z",
          "iopub.status.idle": "2025-03-16T20:27:53.952701Z",
          "shell.execute_reply": "2025-03-16T20:27:53.951790Z",
          "shell.execute_reply.started": "2025-03-16T20:27:50.949302Z"
        },
        "id": "Pw7f2ghccuoK",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "\n",
        "# from tokenizers import Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.corpus import cmudict\n",
        "pronouncing_dict = cmudict.dict()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYzk5LW-vwHD",
        "outputId": "09248983-b9a4-4989-9f4b-e5b393a529ce"
      },
      "outputs": [],
      "source": [
        "word = \"hello\"\n",
        "phonemes = pronouncing_dict.get(word.lower(), [\"Not found\"])\n",
        "print(phonemes)  # Output: [['HH', 'AH0', 'L', 'OW1']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4Qbwuf2bL_xK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def text_to_phonemes(word):\n",
        " \n",
        "    word = word.lower()\n",
        "    if word in pronouncing_dict:\n",
        "        return pronouncing_dict[word][0]  # Take the first pronunciation if multiple exist\n",
        "    else:\n",
        "        return None  # Handle OOV words separately\n",
        "phoneme_vocab = set()\n",
        "for word, phonemes in pronouncing_dict.items():\n",
        "    for phoneme_seq in phonemes:\n",
        "        phoneme_vocab.update(phoneme_seq)\n",
        "\n",
        "phoneme_vocab = sorted(list(phoneme_vocab))  # Sort for consistent indexing\n",
        "phoneme_vocab.insert(0, '[PAD]')\n",
        "phoneme_vocab.insert(1, '[EOS]')\n",
        "phoneme_vocab.insert(2, '[UNK]')\n",
        "phoneme_to_id = {phoneme: idx for idx, phoneme in enumerate(phoneme_vocab)}\n",
        "\n",
        "vocab_size = len(phoneme_vocab)  # Number of unique phonemes\n",
        "\n",
        "def phonemes_to_indices(phoneme_seq):\n",
        "    \n",
        "    return [phoneme_to_id[p] for p in phoneme_seq if p in phoneme_to_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwblB-OhNUJH",
        "outputId": "71ec3364-2f36-4119-8bde-9448a67394ab"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "8e81d8cb-2c43-4550-a8f8-09815ff2a1ab",
        "_uuid": "fe2c17e3-b1bf-4b93-8760-de26e849e84f",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:27:53.954751Z",
          "iopub.status.busy": "2025-03-16T20:27:53.954400Z",
          "iopub.status.idle": "2025-03-16T20:27:56.417272Z",
          "shell.execute_reply": "2025-03-16T20:27:56.416335Z",
          "shell.execute_reply.started": "2025-03-16T20:27:53.954727Z"
        },
        "id": "LwR5_uvTcuoL",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# import re\n",
        "HF_TOKEN = '...'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UrQBMMqqHMV",
        "outputId": "7ba8e982-7f03-4701-f6eb-88598eee37a9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D7AP219KJzTs"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "epochs=10\n",
        "block_size = 80\n",
        "batch_size = 32\n",
        "# src_vocab_size = None\n",
        "src_vocab_size = vocab_size\n",
        "phenome_embeddings_dims = 512\n",
        "embeddings_dims = phenome_embeddings_dims\n",
        "prenet_encoder_embeddings_dims = 512\n",
        "attn_dropout = 0.1\n",
        "no_of_heads = 4 #IMP needs to be thoroughly calculated\n",
        "dropout = 0.1\n",
        "# epochs = 3\n",
        "max_lr = 6e-4\n",
        "no_of_decoder_layers = 8 #IMP needs to be thoroughly calculated\n",
        "attn_dropout = 0.1\n",
        "weight_decay_optim = 0.01\n",
        "log_mel_features = 80\n",
        "kernel_size = 5\n",
        "stride = (2,10)\n",
        "sr = 16000\n",
        "device= 'cuda:0'\n",
        "SAMPLING_RATE=16000\n",
        "N_MELS = 80  # 80-channel Mel spectrogram\n",
        "WINDOW_DURATION = 0.050  # 25 milliseconds\n",
        "STRIDE_DURATION = 0.0125  # 10 milliseconds\n",
        "max_t = 512\n",
        "n_channels = N_MELS\n",
        "clip = 1.0\n",
        "embeddings_dims_decoder = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "_cell_guid": "5ac94789-1d5e-4ace-88f3-25b5413fb78b",
        "_uuid": "92a80d68-b0eb-4f1d-b988-7f190e0b934a",
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:08.020631Z",
          "iopub.status.busy": "2025-03-16T20:28:08.020054Z",
          "iopub.status.idle": "2025-03-16T20:28:08.043556Z",
          "shell.execute_reply": "2025-03-16T20:28:08.042503Z",
          "shell.execute_reply.started": "2025-03-16T20:28:08.020565Z"
        },
        "id": "TmPkI_UEpvor",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1777fa9a-7686-4b62-93c1-c31c2ccf73c1",
        "_uuid": "e08f94e6-dae5-42f3-a285-c151086b8f1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:08.045189Z",
          "iopub.status.busy": "2025-03-16T20:28:08.044805Z",
          "iopub.status.idle": "2025-03-16T20:28:14.373089Z",
          "shell.execute_reply": "2025-03-16T20:28:14.372056Z",
          "shell.execute_reply.started": "2025-03-16T20:28:08.045149Z"
        },
        "id": "IME1Ls95Y3gl",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b2bfc2f2-1ea7-49f4-d5ac-c218b69aaada",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets\n",
        "from tabnanny import verbose\n",
        "from datasets import load_dataset\n",
        "\n",
        "gs = load_dataset(\"keithito/lj_speech\", token=HF_TOKEN)\n",
        "\n",
        "\n",
        "print(gs)\n",
        "\n",
        "\n",
        "audio_input = gs['train'][0][\"audio\"]\n",
        "transcription = gs[\"train\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c018b51-6161-49d9-995e-21cf09d1391f",
        "_uuid": "027b3fe3-8e01-40c4-99de-1a1b12d93453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "d8dfab16b05042d8a652fc7e07d490b3",
            "6f00ac5858a1433d82b8bb674bb15806",
            "1b41965020b4427cb4dd1f5940c6548b",
            "a97d1d83e9b844f6b5abf56c65e9f10d",
            "fe19461da9f2446e8c8efa197a768e7b",
            "5547fead989b48749d3f3ff5d318183d",
            "8f8b148b1cbb4f69912207102fca9963",
            "f3fc312c9bbb4e039461a8e0ddd0374c",
            "fa6360fe2bc948ef9f0e736e3fb87505",
            "d2e2a772b1ab4067a2ab4ba1705a14aa",
            "ad84230998594f73990b7c624c6a0108",
            "1dafc18efead4358a80bc3789996ab33",
            "fe90efd2539741bfaacb9ceaed43e872",
            "58b7e908d0e54832be005b9a7d0dd77f",
            "84e1407eeb034f48b9e704f2749602d6",
            "ddecc5d0c2194d7b97e20ba1d71ec761",
            "601e30955235449a8f1a280f8651a3ae",
            "728d3a6b84a94df58a10059e23169b01",
            "e1d16c1b4781404bbffa4b1ac88b94b1",
            "972978dd76154717aa9b260912d2fa27",
            "b68d91b41ac24242960017dd03cd3d69",
            "988686ba5ddb4dc08b448ea9b52c7e82",
            "62aadffd2d8c4122b57d14d05dfa45f8",
            "208e914165d548ca8a7a9f3a47fbab8e",
            "19a6cd21c0f34ea08d312a5a9f7ef255",
            "b7f991e805ec43069443edbd3b7248b3",
            "cba9d46844a84c5b90c94c2c444c8884",
            "90df26f7d0204a7aab1917718539f5e4",
            "10a6c531b06b426bb35912659a553ffa",
            "58564f61bfc4481c80bbf96654c4dcbd",
            "1485d40f2cc0456da7fda87107743729",
            "4520875846b64b15a448e89ca2641168",
            "c1577849122342e0bcd8376959561965",
            "1199f8b30d444865841804d062065b80",
            "43c70137107a4012add6e83bf3908284",
            "6124f997ce4043f58545d85e5c9dadc0",
            "96211d16cf1148519b3f6d9207e8521a",
            "4c5d4c86a24f4c65b562b7abd923e0c5",
            "7a0777751f3a4ace823ec46a2cf83fb4",
            "f01bb9296c3549f98ca7a42a6c01e911",
            "76790a43f9ad440192f3b83c7755b1ad",
            "d18452f1acb846459e9b95e7d28c1c70",
            "905fa0d1daab44c1981fe39b14a473de",
            "13c306c962f64bedb0d279a140b42594"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T20:28:14.374940Z",
          "iopub.status.busy": "2025-03-16T20:28:14.374239Z",
          "iopub.status.idle": "2025-03-16T20:35:37.108931Z",
          "shell.execute_reply": "2025-03-16T20:35:37.108126Z",
          "shell.execute_reply.started": "2025-03-16T20:28:14.374907Z"
        },
        "id": "cRV1EOlVY3gm",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "74f8c682-bc88-459d-d597-95f793adbe86",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_DURATION_IN_SECONDS = 10\n",
        "\n",
        "gs = gs['train'].train_test_split(test_size=0.2)\n",
        "# print(dataset)\n",
        "# train_data, val_data = dataset['train'], dataset['test']\n",
        "\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS\n",
        "\n",
        "train_new_column = []\n",
        "\n",
        "for x in tqdm(range(len(gs['train']))):\n",
        "    train_new_column.append(librosa.get_duration(path=gs['train'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['train'].add_column(\"duration\", train_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "truncated_gs_train = gs_\n",
        "# truncated_gs_train = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs\n",
        "\n",
        "\n",
        "\n",
        "val_new_column = []\n",
        "# new_column = [librosa.get_duration(path=x) ]]\n",
        "for x in tqdm(range(len(gs['test']))):\n",
        "    val_new_column.append(librosa.get_duration(path=gs['test'][x]['audio']['path']))\n",
        "\n",
        "gs_ = gs['test'].add_column(\"duration\", val_new_column)\n",
        "\n",
        "\n",
        "gs_ = gs_.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "truncated_gs_val = gs_\n",
        "# truncated_gs_val = gs_.remove_columns([\"duration\"])\n",
        "# truncated_gs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68c3295a-6eef-4874-acaf-73f9279fad98",
        "_uuid": "d988499f-ad5b-47cf-bd9e-257395584889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T20:35:37.111778Z",
          "iopub.status.busy": "2025-03-16T20:35:37.111522Z",
          "iopub.status.idle": "2025-03-16T21:17:23.216508Z",
          "shell.execute_reply": "2025-03-16T21:17:23.215522Z",
          "shell.execute_reply.started": "2025-03-16T20:35:37.111754Z"
        },
        "id": "6NZ9Hbp5q1to",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "a93748e4-2a25-40de-bf8e-fbb78e4bd64e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_fft = int(WINDOW_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "hop_length = int(STRIDE_DURATION * MAX_DURATION_IN_SECONDS * SAMPLING_RATE)\n",
        "\n",
        "train_outputs = []\n",
        "train_texts = []\n",
        "train_duration = []\n",
        "val_outputs = []\n",
        "val_texts = []\n",
        "val_duration = []\n",
        "# train_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_train))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_train[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  train_outputs.append(S_dB)\n",
        "  train_texts.append(truncated_gs_train[i]['normalized_text'])\n",
        "  train_duration.append(truncated_gs_train[i]['duration'])\n",
        "\n",
        "val_outputs = []\n",
        "val_texts = []\n",
        "for i in tqdm(range(len(truncated_gs_val))):\n",
        "  S = librosa.feature.melspectrogram(\n",
        "      y=truncated_gs_val[i]['audio']['array'],\n",
        "      sr=SAMPLING_RATE,\n",
        "      n_mels=N_MELS,\n",
        "      n_fft=n_fft,\n",
        "      hop_length=hop_length,\n",
        "      win_length=n_fft,\n",
        "      fmax=SAMPLING_RATE // 2\n",
        "  )\n",
        "\n",
        "\n",
        "  S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "  val_outputs.append(S_dB)\n",
        "  val_texts.append(truncated_gs_val[i]['text'])\n",
        "  val_duration.append(truncated_gs_val[i]['duration'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "_cell_guid": "db1f5e13-91c6-4d24-8d31-325796a8a0fd",
        "_uuid": "b456c1a6-6703-4809-a1fb-fc077a5dc8e3",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.219755Z",
          "iopub.status.busy": "2025-03-16T21:17:23.219057Z",
          "iopub.status.idle": "2025-03-16T21:17:23.229394Z",
          "shell.execute_reply": "2025-03-16T21:17:23.228433Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.219721Z"
        },
        "id": "z2aGf6_7xe9S",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "import re\n",
        "# print(round(random.random(), 1))\n",
        "class TTSDataset(Dataset):\n",
        "\n",
        "  def __init__(self, outputs, texts, duration):\n",
        "\n",
        "    self.data = outputs\n",
        "    self.texts = texts\n",
        "    self.max_t = max_t\n",
        "    self.duration = duration\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "\n",
        "  def pad_phoneme_sequence(self, phoneme_seq, max_length):\n",
        "        \"\"\"Pads phoneme sequences to max_length.\"\"\"\n",
        "        pad_token = 0\n",
        "        if len(phoneme_seq) < max_length:\n",
        "            phoneme_seq += [pad_token] * (max_length - len(phoneme_seq))\n",
        "        else:\n",
        "            phoneme_seq = phoneme_seq[:max_length]\n",
        "        return phoneme_seq\n",
        "\n",
        "\n",
        "  def pad_to_max_t(self, spectrogram, max_t):\n",
        "\n",
        "    n_mels, t = spectrogram.shape\n",
        "    if t < max_t:\n",
        "        # Pad with zeros\n",
        "        pad_width = ((0, 0), (0, max_t - t))\n",
        "        spectrogram = np.pad(spectrogram, pad_width, mode='constant')\n",
        "    else:\n",
        "      spectrogram = spectrogram[:, :max_t]\n",
        "\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "  def clean(self, desc):\n",
        "    # Use regex to remove anything between < and >\n",
        "    cleaned_text = re.sub(r'<[^>]*>', '', desc)\n",
        "    return cleaned_text\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "      # SOT = '<|startoftranscript|>'\n",
        "      # EOT = '<|endoftranscript|>'\n",
        "      # transcribe = '<|transcribe|>'\n",
        "      # prev = '<|prev|>'\n",
        "\n",
        "      #stop token\n",
        "\n",
        "\n",
        "\n",
        "      spectrogram = self.pad_to_max_t(self.data[idx], self.max_t)\n",
        "      # probs = round(random.random(),1)\n",
        "      spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
        "      original_frames = int((SAMPLING_RATE / hop_length) * self.duration[idx])\n",
        "      last_frame = min(original_frames, self.max_t) - 1\n",
        "\n",
        "      stop = torch.zeros((N_MELS, max_t), device=device)\n",
        "      stop[:, last_frame] = 1.0\n",
        "      # print(stop\n",
        "      # if(probs == 0.5):\n",
        "        # Normalize the spectrogram between -1 and 1\n",
        "      spectrogram_min = spectrogram.min()\n",
        "      spectrogram_max = spectrogram.max()\n",
        "\n",
        "      # spectrogram = spectrogram.unsqueeze(0)  # Shape: (1, n_mels, max_t)\n",
        "      # prev_text =\n",
        "      # text = self.clean(self.texts[idx])\n",
        "      text = self.texts[idx]\n",
        "      text = text.lower()\n",
        "      # text = SOT  + 'en' + transcribe +  text + EOT\n",
        "      # text += '[EOS]'\n",
        "      # tokenized_text = tokenizer(text, truncation=True, padding='max_length', max_length=block_size, return_tensors='pt')\n",
        "      text = text.split(' ')\n",
        "      phenomes = []\n",
        "      # print(text)\n",
        "      temp = []\n",
        "      # tokenized_text = []\n",
        "      tokenized_text = {}\n",
        "      spectrograms = {}\n",
        "      # for batch in range(batch_size):\n",
        "      for i in range(len(text)):\n",
        "        phenomes_now = text_to_phonemes(text[i])\n",
        "        # print(phenomes_now)\n",
        "        if(phenomes_now == None):\n",
        "          temp = phonemes_to_indices('[UNK]')\n",
        "        else:\n",
        "          temp = phonemes_to_indices(phenomes_now)\n",
        "        phenomes.extend(temp)\n",
        "      # phenomes.extend(phonemes_to_indices('[EOS]'))\n",
        "      #   tokenized_text.append(phenomes)\n",
        "      #   temp = []\n",
        "      #   phenomes = []\n",
        "\n",
        "      # tokenized_text = torch.stack([tokenized_text])\n",
        "      # print(text)\n",
        "\n",
        "      # print(phenomes)\n",
        "      phenomes = self.pad_phoneme_sequence(phenomes, block_size)\n",
        "      # print(phenomes)\n",
        "      tokenized = torch.tensor(phenomes, dtype=torch.long)\n",
        "      # tokenized_text['input_ids'] = tokenized\n",
        "      # print(tokenized_text.shape)\n",
        "\n",
        "      epsilon = 1e-8  # To avoid division by zero\n",
        "      spectrogram = 2 * ((spectrogram - spectrogram_min) / (spectrogram_max - spectrogram_min + epsilon)) - 1\n",
        "\n",
        "      # tokenized_win_prompt = tokenizer(text, max_length = ModelArgs.block_size, padding='max_length', truncation=True,  return_tensors=\"pt\").to(device)\n",
        "      # tokenized_text['labels'] = tokenized_text['input_ids'].clone()\n",
        "      # tokenized_text['labels'][:-1] = tokenized_text['input_ids'][: , 1:]\n",
        "      # tokenized_text['labels'][: , -1] = phonemes_to_indices('[EOS]')\n",
        "\n",
        "      # tokenized_text_x = tokenized_text['input_ids'].squeeze(0)\n",
        "      # tokenized_text_y = tokenized_text['labels'].squeeze(0)\n",
        "\n",
        "      # print(tokenized_text.shape)\n",
        "      # print(\"dataset: \", tokenized_text)\n",
        "      spectrograms['input_ids'] = spectrogram\n",
        "      spectrograms['labels'] = spectrogram\n",
        "      # stop_tokens = tokenized\n",
        "      return spectrograms, tokenized, stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BWiwc8a5Qkhe"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    text = []\n",
        "    input_ids_list = []\n",
        "    labels_list = []\n",
        "    stop = []\n",
        "    for spec, text_dict, stop_token in batch:\n",
        "\n",
        "        # spectrograms.append(spec)\n",
        "\n",
        "        stop.append(stop_token)\n",
        "        input_ids_list.append(spec['input_ids'])\n",
        "        labels_list.append(spec['labels'])\n",
        "        text.append(text_dict)\n",
        "\n",
        "    # 3. Stack tensors\n",
        "    text = torch.stack(text)\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    labels = torch.stack(labels_list)\n",
        "    stop = torch.stack(stop)\n",
        "    # 4. Return in proper format\n",
        "    return {\n",
        "        'text': text,\n",
        "        'input_ids': input_ids,\n",
        "        'labels': labels,\n",
        "        \"stop_tokens\": stop\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78e27c8d-e3cd-434c-b2c6-9d16b79eb5be",
        "_uuid": "5ca81223-5c7e-43e5-83b2-7b00a3ecee7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.230758Z",
          "iopub.status.busy": "2025-03-16T21:17:23.230437Z",
          "iopub.status.idle": "2025-03-16T21:17:23.258809Z",
          "shell.execute_reply": "2025-03-16T21:17:23.257804Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.230731Z"
        },
        "id": "2FJvDV-4psOo",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b053ceea-5521-4f12-85fc-9abe2908bf84",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.autograd.set_detect_anomaly(True)  # Add at the start of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "_cell_guid": "d26cf6eb-55b2-484c-bc9b-10952ea5c977",
        "_uuid": "8db84c70-d655-4b7c-8c00-9e195d240a7d",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:23.260278Z",
          "iopub.status.busy": "2025-03-16T21:17:23.259908Z",
          "iopub.status.idle": "2025-03-16T21:17:23.478760Z",
          "shell.execute_reply": "2025-03-16T21:17:23.477736Z",
          "shell.execute_reply.started": "2025-03-16T21:17:23.260242Z"
        },
        "id": "2tvMuBSOynPy",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "shuffle = True\n",
        "\n",
        "train_dataset = TTSDataset(train_outputs, train_texts, train_duration)\n",
        "val_dataset = TTSDataset(val_outputs, val_texts, val_duration)\n",
        "\n",
        "generator = torch.Generator(device=device)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    generator=generator,\n",
        "    shuffle=shuffle,\n",
        "     drop_last=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "\n",
        "    generator=generator,\n",
        "    drop_last=True ,\n",
        "    shuffle=False,\n",
        "    collate_fn = collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "j6t9Pb275ZpE"
      },
      "outputs": [],
      "source": [
        "#Position embeddings\n",
        "class SrcPositionEmbeddings(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        block_size = block_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        # nn.init.normal_(self.position_embeddings.weight.data, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "_cell_guid": "a7c6dcaa-5a5d-4260-8696-ee827274edff",
        "_uuid": "25ee96a4-00b2-40a5-a8e4-10ea78a00481",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.575666Z",
          "iopub.status.busy": "2025-03-16T21:17:25.575422Z",
          "iopub.status.idle": "2025-03-16T21:17:25.580222Z",
          "shell.execute_reply": "2025-03-16T21:17:25.579208Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.575643Z"
        },
        "id": "kzXk76ULY3gm",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Position embeddings\n",
        "class TgTPositionEmbeddings(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        block_size = block_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, N_MELS, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        # nn.init.normal_(self.position_embeddings.weight.data, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "_cell_guid": "779e1bc7-798e-4e86-ad9e-0967b08dcab9",
        "_uuid": "6c495fe4-d351-4477-be75-b575ea534db0",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.581153Z",
          "iopub.status.busy": "2025-03-16T21:17:25.580958Z",
          "iopub.status.idle": "2025-03-16T21:17:25.631084Z",
          "shell.execute_reply": "2025-03-16T21:17:25.630436Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.581135Z"
        },
        "id": "-p1sURm3psOp",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# pos = PositionEmbeddings()\n",
        "# x = torch.randn(batch_size, block_size, embeddings_dims)\n",
        "# pos(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0slFOCCu7gYz"
      },
      "outputs": [],
      "source": [
        "class PrenetEncoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.embeds_dims = phenome_embeddings_dims\n",
        "    self.out = prenet_encoder_embeddings_dims\n",
        "    self.conv1d_layer1 = nn.Conv1d(in_channels=self.embeds_dims, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv1d_layer2 = nn.Conv1d(in_channels=self.out, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv1d_layer3 = nn.Conv1d(in_channels=self.out, out_channels=self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.norm = torch.nn.BatchNorm1d(self.out, device=self.device)\n",
        "    self.proj = nn.Linear(self.out, self.out, device=self.device)\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.conv1d_layer1(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.conv1d_layer2(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    # x = torch.nn.functional.gelu(x)\n",
        "    x = self.conv1d_layer3(x)\n",
        "    # print(\"x now: \", x.shape)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    # print(\"x shape: \", x.shape)\n",
        "    x = x.permute(0,2,1)\n",
        "    x = self.proj(x)\n",
        "    # print(x.shape)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CWSKPVpN0hZg"
      },
      "outputs": [],
      "source": [
        "class PrenetDecoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.decoder_embeds_dims = embeddings_dims_decoder\n",
        "    self.device = device\n",
        "    self.out = phenome_embeddings_dims\n",
        "    self.linear_layer1 = nn.Linear(in_features=max_t, out_features=self.decoder_embeds_dims, device=self.device)\n",
        "    self.linear_layer2 = nn.Linear(in_features=self.decoder_embeds_dims, out_features=self.out, device=self.device)\n",
        "    self.linear_layer3 = nn.Linear(self.out, self.out, device=self.device)\n",
        "  def forward(self, x):\n",
        "    x = self.linear_layer1(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.linear_layer2(x)\n",
        "    x = torch.nn.functional.relu(x)\n",
        "    x = self.linear_layer3(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "_cell_guid": "0e795d31-2801-446a-807d-0ea3bebb6e3d",
        "_uuid": "ae3226bb-4414-4840-8da7-73db14ad8271",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.637345Z",
          "iopub.status.busy": "2025-03-16T21:17:25.637118Z",
          "iopub.status.idle": "2025-03-16T21:17:25.653427Z",
          "shell.execute_reply": "2025-03-16T21:17:25.652838Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.637325Z"
        },
        "id": "REUDHWrWcuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Layer Normalization\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embeddings_dims = embeddings_dims\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(normalized_shape=embeddings_dims)\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "_cell_guid": "762624a5-8d63-403b-bc4e-039b16aba3f9",
        "_uuid": "70301094-ba8a-4078-a1ad-46dc8d357f6a",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.654302Z",
          "iopub.status.busy": "2025-03-16T21:17:25.654077Z",
          "iopub.status.idle": "2025-03-16T21:17:25.672472Z",
          "shell.execute_reply": "2025-03-16T21:17:25.671677Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.654281Z"
        },
        "id": "lEe02cH9cuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#FeedForward Neural Network\n",
        "\n",
        "class MLPBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dropout = dropout,\n",
        "        embeddings_size = embeddings_dims,\n",
        "        # inner_dimensional_states: int = 3072\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(device=device, in_features=embeddings_size, out_features= 4 * embeddings_dims),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(device=device, in_features= 4 * embeddings_dims, out_features=embeddings_size),\n",
        "            nn.Dropout(p = dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mlp_weights_init = self.mlp.apply(weights_init)\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "_cell_guid": "50c3f233-f3ea-44cc-bccf-10aa13fc35d5",
        "_uuid": "bd1f493a-c5eb-4821-938b-1fd4baf55549",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.673679Z",
          "iopub.status.busy": "2025-03-16T21:17:25.673386Z",
          "iopub.status.idle": "2025-03-16T21:17:25.693459Z",
          "shell.execute_reply": "2025-03-16T21:17:25.692644Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.673647Z"
        },
        "id": "cf0Jf_7UcuoN",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class MaskedAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # print(embeddings_dims)\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
        "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        weights_normalized = self.dropout(weights_normalized)\n",
        "        out = weights_normalized @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "_cell_guid": "aee629bd-84ad-4f0c-a0e7-9e3070e10081",
        "_uuid": "a252848c-a0b1-4c08-8a42-e2851e642a06",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.694740Z",
          "iopub.status.busy": "2025-03-16T21:17:25.694382Z",
          "iopub.status.idle": "2025-03-16T21:17:25.713823Z",
          "shell.execute_reply": "2025-03-16T21:17:25.712953Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.694708Z"
        },
        "id": "OUFERSL2u8LT",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class MaskedMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([MaskedAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x):\n",
        "        concat = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "_cell_guid": "76ee1dab-07cf-4ae4-ac9a-3c134b9dc41d",
        "_uuid": "0bdd11ba-b65a-4dbe-9b09-25d24c2b3ab6",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.714859Z",
          "iopub.status.busy": "2025-03-16T21:17:25.714607Z",
          "iopub.status.idle": "2025-03-16T21:17:25.736087Z",
          "shell.execute_reply": "2025-03-16T21:17:25.735469Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.714838Z"
        },
        "id": "oGGyyF4pjHyd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Single Attention Head\n",
        "\n",
        "class CrossAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "\n",
        "\n",
        "        batch, block_size, embd_dims = query.shape\n",
        "        q = self.query(query)\n",
        "        k = self.keys(key)\n",
        "        v = self.values(value)\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        # weights = query @ torch.transpose(key, dim0=-2, dim1=-1) * (key.shape[-1] ** -0.5)\n",
        "        # if(mask != None):\n",
        "        #     mask = mask.unsqueeze(1)\n",
        "        #     masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "        #     weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        #     # weights_normalized = self.dropout(weights_normalized)\n",
        "        #     out = weights_normalized @ value\n",
        "        #     out = self.dropout(out)\n",
        "        #     return out\n",
        "        # else:\n",
        "        #     weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        #     # weights_normalized = self.dropout(weights_normalized)\n",
        "        #     out = weights_normalized @ value\n",
        "        #     out = self.dropout(out)\n",
        "        #     return out\n",
        "\n",
        "        masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
        "        weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "        weights_normalized = self.dropout(weights_normalized)\n",
        "        out = weights_normalized @ v\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "_cell_guid": "bcde57b5-61a0-4d56-aa5a-5fbbc2f2a9bd",
        "_uuid": "af55eb4d-6d72-465f-8692-4777da91e56f",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.737123Z",
          "iopub.status.busy": "2025-03-16T21:17:25.736929Z",
          "iopub.status.idle": "2025-03-16T21:17:25.763686Z",
          "shell.execute_reply": "2025-03-16T21:17:25.762859Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.737106Z"
        },
        "id": "U5NmszzcjHyf",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Single Attention Head\n",
        "\n",
        "class FullAttentionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.head_size = embeddings_dims // no_of_heads\n",
        "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device, bias=False)\n",
        "        self.keys = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,device=device, bias=False)\n",
        "        self.values = nn.Linear(in_features=embeddings_dims, out_features=self.head_size, device=device,bias=False)\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # batch, block_size, embd_dims = x.shape\n",
        "        k = self.keys(x)\n",
        "        q = self.query(x)\n",
        "        v = self.values(x)\n",
        "        # masked_table = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "        weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
        "        if(mask != None):\n",
        "            mask = mask.unsqueeze(1)\n",
        "            masked_values = weights.masked_fill(mask == 0, float('-inf'))\n",
        "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out\n",
        "        else:\n",
        "            weights_normalized = nn.functional.softmax(weights, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
        "            # weights_normalized = self.dropout(weights_normalized)\n",
        "            out = weights_normalized @ v\n",
        "            out = self.dropout(out)\n",
        "            return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "_cell_guid": "8099497c-1f19-4d53-8965-ae8f153335be",
        "_uuid": "50fee96b-7dae-4da2-951a-b06330293a3e",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.765002Z",
          "iopub.status.busy": "2025-03-16T21:17:25.764640Z",
          "iopub.status.idle": "2025-03-16T21:17:25.787092Z",
          "shell.execute_reply": "2025-03-16T21:17:25.786217Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.764968Z"
        },
        "id": "v_BB7r7kqmOc",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class FullMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([FullAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # 12 (no of heads) * (batch_size) 64 = 768 -> gives out the text embeddings\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        concat = torch.cat([head(x, mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "_cell_guid": "06ee6670-5428-4e5a-867e-cdd98cddc2aa",
        "_uuid": "3627fa0f-bd35-4dab-9451-03a2b716732b",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.788020Z",
          "iopub.status.busy": "2025-03-16T21:17:25.787815Z",
          "iopub.status.idle": "2025-03-16T21:17:25.809259Z",
          "shell.execute_reply": "2025-03-16T21:17:25.808685Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.788001Z"
        },
        "id": "TTwRkBzcvE-_",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CrossMHA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([CrossAttentionHead(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads) for _ in range(no_of_heads)])\n",
        "        self.dropout = nn.Dropout(p = attn_dropout)\n",
        "        self.linear = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False)\n",
        "\n",
        "    def forward(self, value, key, x, mask=None):\n",
        "        concat = torch.cat([head(x, key, value,  mask) for head in self.heads], dim=-1)\n",
        "        linear_layer = self.linear(concat)\n",
        "        out = self.dropout(linear_layer)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "_cell_guid": "2e095386-2ee5-4a3c-83bb-cad96cb26746",
        "_uuid": "2c290085-c1fe-4c64-9bab-baa448c3bae5",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.810295Z",
          "iopub.status.busy": "2025-03-16T21:17:25.810050Z",
          "iopub.status.idle": "2025-03-16T21:17:25.826116Z",
          "shell.execute_reply": "2025-03-16T21:17:25.825190Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.810261Z"
        },
        "id": "s9rJzO_XcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross = CrossMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.masked = MaskedMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        # self.layer_norm3 = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.layer_norm4 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, key, value, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.masked(x)) #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        # print(x.shape)\n",
        "        x = self.layer_norm2(x + self.cross(value, key, x, mask)) #Very important step\n",
        "        # print(x.shape)\n",
        "        # x = x + self.mha(self.layer_norm1(x))  #Very important step -> Layer Norm on input and then passes it to the subsequent blocks\n",
        "        x = self.layer_norm4(x + self.mlp_block(x)) #Very important step\n",
        "        # print(x.shape)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PjbaRskFRHkN"
      },
      "outputs": [],
      "source": [
        "class PostNet(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.out = embeddings_dims\n",
        "    self.device = device\n",
        "    self.conv_layer1 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv_layer2 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv_layer3 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv_layer4 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.conv_layer5 = nn.Conv1d(self.out, self.out, kernel_size=kernel_size, device=self.device, padding=2)\n",
        "    self.norm = torch.nn.BatchNorm1d(self.out)\n",
        "    # self.norm2 = torch.nn.BatchNorm1d(N_MELS)\n",
        "  def forward(self,x):\n",
        "    # print(\"here: \", x.shape)\n",
        "    x = x.transpose(1,2).contiguous()\n",
        "    x = self.conv_layer1(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer2(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer3(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer4(x)\n",
        "    x = self.norm(x)\n",
        "    x = torch.nn.functional.tanh(x)\n",
        "    x = self.conv_layer5(x)\n",
        "    x = self.norm(x)\n",
        "    x = x.transpose(1,2).contiguous()\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "_cell_guid": "63e5400a-b5d2-4ef7-8f30-b1bc0e233820",
        "_uuid": "05fd4f49-cc4b-474c-9a9c-3d5191eefa43",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.827199Z",
          "iopub.status.busy": "2025-03-16T21:17:25.826969Z",
          "iopub.status.idle": "2025-03-16T21:17:25.850583Z",
          "shell.execute_reply": "2025-03-16T21:17:25.849957Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.827178Z"
        },
        "id": "KGh8ujQJcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder Block\n",
        "\n",
        "class DecoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=tgt_vocab_size, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.layer_norm = LayerNormalization(embeddings_dims=embeddings_dims)\n",
        "        self.decoder_layers = nn.ModuleList([TransformerDecoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "        # self.positional_embeddings_tgt = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        self.positional_embeddings_tgt = TgTPositionEmbeddings()\n",
        "        self.scaled_factor = nn.Parameter(torch.ones(1, N_MELS, embeddings_dims), requires_grad=True)\n",
        "        # torch.nn.init.normal_(self.positional_embeddings_tgt, mean=0.0, std=0.02)\n",
        "\n",
        "        # out = self.decoder_layers(query, key, x)\n",
        "        # Loop through each decoder layer\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, key, value, x, mask):\n",
        "        # x = self.tgt_text_embds(x)\n",
        "        # print(x.shape)\n",
        "        x = x + self.scaled_factor * self.positional_embeddings_tgt(x)\n",
        "        # print(x.shape)\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(key, value, x, mask)\n",
        "        # x = self.layer_norm(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "_cell_guid": "5327372f-0db3-4ef1-a59c-1757b6edfc62",
        "_uuid": "1e94a41d-aaa3-4e74-a02f-fb0edbfa9118",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.854582Z",
          "iopub.status.busy": "2025-03-16T21:17:25.854371Z",
          "iopub.status.idle": "2025-03-16T21:17:25.876149Z",
          "shell.execute_reply": "2025-03-16T21:17:25.875339Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.854563Z"
        },
        "id": "A3SgKrC-jHyd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "_cell_guid": "2a3683d1-303c-41a2-9cc6-b7e053fde34c",
        "_uuid": "3ecc13cd-e38a-4e1c-af9c-10f8e84b02d7",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.878065Z",
          "iopub.status.busy": "2025-03-16T21:17:25.877823Z",
          "iopub.status.idle": "2025-03-16T21:17:25.893369Z",
          "shell.execute_reply": "2025-03-16T21:17:25.892523Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.878033Z"
        },
        "id": "v6mbbO3yp-gh",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        dropout = dropout,\n",
        "        mask=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = FullMHA(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads)\n",
        "        self.layer_norm1 = LayerNormalization(embeddings_dims)\n",
        "        self.layer_norm2 = LayerNormalization(embeddings_dims)\n",
        "        self.mlp_block = MLPBlock(dropout=dropout, embeddings_size=embeddings_dims)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.layer_norm1(x + self.mha(x, mask))\n",
        "        x = self.layer_norm2(x + self.mlp_block(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "_cell_guid": "2fa82426-e551-497d-8247-051a55f74a3c",
        "_uuid": "39475274-a828-4151-9556-fa4bf30d4455",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.894660Z",
          "iopub.status.busy": "2025-03-16T21:17:25.894341Z",
          "iopub.status.idle": "2025-03-16T21:17:25.916685Z",
          "shell.execute_reply": "2025-03-16T21:17:25.915829Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.894605Z"
        },
        "id": "HxW0pvnV12Ms",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class EncoderModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        attn_dropout = attn_dropout,\n",
        "        embeddings_dims = embeddings_dims,\n",
        "        no_of_heads = no_of_heads,\n",
        "        block_size = block_size,\n",
        "        dropout = dropout,\n",
        "        no_of_decoder_layers = no_of_decoder_layers,\n",
        "        # vocab_size = vocab_size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.positional_embeddings_src = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True) #To give positional embeddings to each token of the input text, hence num_embeddings=block_size\n",
        "        self.prenet_enc = PrenetEncoder()\n",
        "        # self.pos_embeds = nn.Parameter(torch.randn(1, block_size, embeddings_dims, device=device), requires_grad=True)\n",
        "        self.trainable_factor = nn.Parameter(torch.ones(1, block_size, embeddings_dims, device=device), requires_grad=True)\n",
        "        # self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "        # self.conv2 = nn.Conv1d(in_channels=embeddings_dims, out_channels=embeddings_dims, kernel_size=kernel_size, device=device, padding=1)\n",
        "\n",
        "        self.positional_embeddings_src = SrcPositionEmbeddings()\n",
        "        self.src_text_embeds = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim=embeddings_dims, device=device)\n",
        "        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.encoder_layers = nn.ModuleList([TransformerEncoderBlock(attn_dropout=attn_dropout, embeddings_dims=embeddings_dims, no_of_heads=no_of_heads, dropout=dropout) for _ in range(no_of_decoder_layers)])\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):  #Weight Initialization\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "\n",
        "        # x = self.conv1(x)\n",
        "        # x = torch.nn.functional.gelu(x)\n",
        "        # x = self.conv2(x)\n",
        "        # x = torch.nn.functional.gelu(x)\n",
        "        # print(x.shape)\n",
        "        x = self.src_text_embeds(x)\n",
        "        # print(self.positional_embeddings_src.shape)\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        x = self.prenet_enc(x)\n",
        "        # x = x.permute(0, 2, 1)\n",
        "        # print(x.shape)\n",
        "        # print(self.positional_embeddings_src(x).shape)\n",
        "        x = x + self.trainable_factor * self.positional_embeddings_src(x)\n",
        "        # print(x)\n",
        "        # print(x.shape)\n",
        "        # Loop through each encoder layer\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "_cell_guid": "a6d4d773-0ed2-4059-ab2b-683692ce2c32",
        "_uuid": "6f0e32f9-14f2-4b65-a106-cab2051ad125",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.917942Z",
          "iopub.status.busy": "2025-03-16T21:17:25.917671Z",
          "iopub.status.idle": "2025-03-16T21:17:25.937565Z",
          "shell.execute_reply": "2025-03-16T21:17:25.936953Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.917913Z"
        },
        "id": "2UWijIFl2Ykd",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class TTS(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = EncoderModel()\n",
        "        self.decoder = DecoderModel()\n",
        "        self.postnet = PostNet()\n",
        "        # self.pos = PositionalEmbeddings()\n",
        "        # self.tgt_text_embds = TgtTextEmbeddings(vocab_size=tgt_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False) # Takes in logits of dimensions- embeds_dims and converts it into dimension of vocab_size (logits in range of vocab_size)\n",
        "        # self.src_text_embeds = SrcTextEmbeddings(vocab_size=src_vocab_size, embeddings_dims=embeddings_dims)\n",
        "        self.stop_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims, device=device, bias=False)\n",
        "        self.prenet_dec = PrenetDecoder()\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        # x = self.src_text_embeds(src)\n",
        "        x = self.encoder(src, src_mask)\n",
        "        # print(tgt.shape)\n",
        "        y = self.prenet_dec(tgt)\n",
        "        # y = self.tgt_text_embds(tgt)\n",
        "        # print(x.shape)\n",
        "        y = self.decoder(x, x, y, tgt_mask)\n",
        "        # print(y.shape)\n",
        "\n",
        "        out1 = self.linear_layer(y)\n",
        "        out2 = self.postnet(out1)\n",
        "        out2 += out1\n",
        "        stop = self.stop_layer(y)\n",
        "        # stop = torch.nn.functional.sigmoid(stop)\n",
        "        return out1, out2, stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "_cell_guid": "17dd187f-c195-44a0-990c-c6d17c90227e",
        "_uuid": "e407283d-b631-479c-9bae-014aab7a0a3d",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:25.938678Z",
          "iopub.status.busy": "2025-03-16T21:17:25.938378Z",
          "iopub.status.idle": "2025-03-16T21:17:26.442178Z",
          "shell.execute_reply": "2025-03-16T21:17:26.441341Z",
          "shell.execute_reply.started": "2025-03-16T21:17:25.938647Z"
        },
        "id": "ntIaQj1U3pFX",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Instantiating the model\n",
        "model = TTS()\n",
        "# model = torch.compile(model)\n",
        "# model = model.to(device)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "CkU_ZBCFipMF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(text.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d92d3629-d59c-4361-9572-8e0075329808",
        "_uuid": "5061d499-36a9-427b-90ad-eead117d5a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T21:17:26.759979Z",
          "iopub.status.busy": "2025-03-16T21:17:26.759689Z",
          "iopub.status.idle": "2025-03-16T21:17:31.629859Z",
          "shell.execute_reply": "2025-03-16T21:17:31.628564Z",
          "shell.execute_reply.started": "2025-03-16T21:17:26.759955Z"
        },
        "id": "yOXtmG-lcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "02e765a7-ee88-4980-df4f-bc25d170d26d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "data = next(iter(train_dataloader))\n",
        "# print(data)\n",
        "# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "spec1 = data['input_ids'].to(device)\n",
        "text = data['text'].to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_data=(text, spec1),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "_cell_guid": "97226398-cb33-48d6-9abc-f5a9c549b5c5",
        "_uuid": "ee98acba-6992-4b1e-8cc1-3eda3997ae5b",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:47.480394Z",
          "iopub.status.busy": "2025-03-16T21:18:47.480025Z",
          "iopub.status.idle": "2025-03-16T21:18:47.490347Z",
          "shell.execute_reply": "2025-03-16T21:18:47.489487Z",
          "shell.execute_reply.started": "2025-03-16T21:18:47.480363Z"
        },
        "id": "LH95cJEvcuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Optimizer setup and scheduler steup\n",
        "# out = {\"Train\": None, \"val\": None}\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr)\n",
        "\n",
        "\n",
        "loss_fn = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "_cell_guid": "53e031da-134a-4271-8879-b012f262ccc6",
        "_uuid": "7860931f-c816-4796-b4c5-265e3df9bf57",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:47.977592Z",
          "iopub.status.busy": "2025-03-16T21:18:47.977290Z",
          "iopub.status.idle": "2025-03-16T21:18:47.981528Z",
          "shell.execute_reply": "2025-03-16T21:18:47.980718Z",
          "shell.execute_reply.started": "2025-03-16T21:18:47.977569Z"
        },
        "id": "bbvONdUTWmvL",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "scaler = torch.amp.GradScaler(enabled=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "_cell_guid": "58095701-5ad6-4bd6-bc30-0b1422af2a93",
        "_uuid": "6d9f8456-d5da-49d1-8211-c97897782909",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.014114Z",
          "iopub.status.busy": "2025-03-16T21:18:48.013911Z",
          "iopub.status.idle": "2025-03-16T21:18:48.018021Z",
          "shell.execute_reply": "2025-03-16T21:18:48.017242Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.014096Z"
        },
        "id": "MdhqasdjpsOr",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _save_snapshot(model, optimizer, scheduler, epoch, step):\n",
        "    snapshot = {\n",
        "        \"MODEL_STATE\": model.state_dict(),\n",
        "        \"OPTIMIZER_STATE\": optimizer.state_dict(),\n",
        "        # \"SCHEDULER_STATE\": scheduler.state_dict(),\n",
        "        \"EPOCHS_RUN\": epoch,\n",
        "        \"STEP_RUN\": step\n",
        "    }\n",
        "    torch.save(snapshot, f\"snapshot_{step}.pt\")\n",
        "    print(f\"Epoch: {epoch} | Step: {step} | Snapshot saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "_cell_guid": "6f951a8f-3f05-4b6a-9080-99a81e5925b0",
        "_uuid": "9f8791fb-3c63-4871-8402-fbfb2a649e89",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.045122Z",
          "iopub.status.busy": "2025-03-16T21:18:48.044918Z",
          "iopub.status.idle": "2025-03-16T21:18:48.048197Z",
          "shell.execute_reply": "2025-03-16T21:18:48.047524Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.045104Z"
        },
        "id": "wBo4MmRopsOr",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install torchtriton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "_cell_guid": "2fc4d2f7-40d4-4733-8e32-65c38f198b5e",
        "_uuid": "5582f78b-b57d-484e-bfa5-66a2688a6e21",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.069834Z",
          "iopub.status.busy": "2025-03-16T21:18:48.069579Z",
          "iopub.status.idle": "2025-03-16T21:18:48.074196Z",
          "shell.execute_reply": "2025-03-16T21:18:48.073418Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.069814Z"
        },
        "id": "EpyoWzdxpsOr",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "save_chechpoint_iter = 50\n",
        "total_iters = 20000\n",
        "eval_iters = 50\n",
        "eval_check = 100\n",
        "warmup_iters = 2048\n",
        "min_lr = 0.1 * max_lr\n",
        "lr_decay_iters = 20000\n",
        "total_batch_size = 524288\n",
        "micro_batch_size = batch_size\n",
        "gradient_accumulation_steps = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20e917d1-9770-456a-99dd-5e9c33d4edda",
        "_uuid": "a0929ebe-c656-48ba-b479-e08113f3113e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.108479Z",
          "iopub.status.busy": "2025-03-16T21:18:48.108253Z",
          "iopub.status.idle": "2025-03-16T21:18:48.126877Z",
          "shell.execute_reply": "2025-03-16T21:18:48.126223Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.108459Z"
        },
        "id": "oATkWQfApsOs",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "02fc4a9e-0dc9-4303-d762-bb158f897752",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "world_size = torch.cuda.device_count()\n",
        "@torch.inference_mode()\n",
        "def estimate_loss(val_loader, val_iterator, device):\n",
        "    out = {}\n",
        "    # train_loader = prepare_dataset('train', ModelArgs.batch_size)\n",
        "\n",
        "    # val_loader_iterator = iter(val_loader)\n",
        "    loader = None\n",
        "    epoch_loss = None\n",
        "    epoch_losses = []\n",
        "    # print(\"Starting the eval...\")\n",
        "    for split in ['val']:\n",
        "        print(f\"Starting with {split} evaluation...\")\n",
        "        # losses = torch.zeros(ModelArgs.val_epochs)\n",
        "        # if(split == 'train'):\n",
        "        #         loader = train_loader\n",
        "        # if(split == 'val'):\n",
        "        #         loader = val_loader\n",
        "        for step in range(eval_check):\n",
        "            try:\n",
        "                data = next(val_iterator)\n",
        "            except StopIteration:\n",
        "                val_loader_iterator = iter(val_loader)\n",
        "                data = next(val_loader_iterator)\n",
        "\n",
        "            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "            total_loss = 0\n",
        "            # loader.sampler.set_epoch(step)\n",
        "            total_batches = 0\n",
        "            # batch = next(val_loader_iterator)\n",
        "            # for batch in loader:  # Loop through DataLoader batches\n",
        "            # idx = batch['input_ids']\n",
        "            # targets = batch['labels']\n",
        "\n",
        "            data['text'] = data['text'].to(device)\n",
        "            data['input_ids'] = data['input_ids'].to(device)\n",
        "            data['labels'] = data['labels'].to(device)\n",
        "            idx = data['text']\n",
        "            spec = data['input_ids']\n",
        "            y = data['labels']\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "\n",
        "                # pre, post, stop_token = model(idx, spec)\n",
        "                pre, post, _stop = model(idx, spec)\n",
        "                # batch_size, block_size, embeddings_dims = stop_token.shape\n",
        "                # print(\"y: \", y.shape)\n",
        "                # print(\"Pre: \", pre.shape)\n",
        "                # print(\"post: \", post.shape)\n",
        "                # print(\"stop: \", _stop.shape)\n",
        "\n",
        "                # print(logits.shape)\n",
        "                # print(targets)\n",
        "                # stop_token = stop_token.view(batch_size*block_size, embeddings_dims)\n",
        "                # # print(\"OK\")\n",
        "                # targets = idx.view(batch_size * block_size)\n",
        "\n",
        "                # # print(\"OK2\")\n",
        "\n",
        "                pre_loss = nn.functional.mse_loss(pre, y)\n",
        "                post_loss = nn.functional.mse_loss(post, y)\n",
        "\n",
        "                # stop_token = stop_token.view(batch_size*block_size, embeddings_dims)\n",
        "                # print(\"OK\")\n",
        "                # targets = idx.view(batch_size * block_size)\n",
        "\n",
        "                # print(\"OK2\")\n",
        "\n",
        "                # pre_loss = nn.functional.mse_loss(pre, y)\n",
        "\n",
        "                stop_loss = nn.functional.binary_cross_entropy_with_logits( _stop, stop_token, pos_weight = torch.tensor([8.0]))\n",
        "                # print(pre_loss)\n",
        "                # print(post_loss)\n",
        "                # print(stop_loss)\n",
        "                loss = pre_loss + post_loss + stop_loss\n",
        "                total_loss += loss.item()\n",
        "                total_batches += 1\n",
        "\n",
        "        # Compute mean loss for this epoch\n",
        "        epoch_loss = total_loss / total_batches if total_batches > 0 else 0.0\n",
        "        epoch_losses.append(epoch_loss)\n",
        "\n",
        "            # print(f\"Epoch {epoch + 1}/{ModelArgs.val_epochs}: Loss = {epoch_loss:.4f}\")\n",
        "\n",
        "        # Compute mean loss across all evaluation epochs\n",
        "        out[split] = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0\n",
        "        epoch_loss = None\n",
        "        epoch_losses = []\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# model = model.to(rank)\n",
        "model.train()\n",
        "count = 0\n",
        "\n",
        "# train_dataloader = prepare_dataset('train', device, ModelArgs.batch_size)\n",
        "# val_loader= prepare_dataset('val', device, ModelArgs.batch_size)\n",
        "# for step in tqdm(range(total_iters)):\n",
        "# for epoch in range(ModelArgs.epochs):\n",
        "    # torch.cuda.synchronize()\n",
        "\n",
        "# train_dataloader.sampler.set_epoch(epoch)\n",
        "\n",
        "# val_loader.sampler.set_epoch(epoch)\n",
        "print(\"Loaders ready both\")\n",
        "epochs = epochs\n",
        "\n",
        "# train_step_iterator = range(len(train_dataloader))\n",
        "# if device == 0:  # Only create progress bar on rank 0\n",
        "#   train_step_iterator = tqdm(train_step_iterator, desc=\"Training Progress\", position=0, leave=True)\n",
        "\n",
        "    # Print progress on rank 0\n",
        "train_loader_length = 0\n",
        "train_data_iterator = iter(train_dataloader)\n",
        "val_data_iterator = iter(val_dataloader)\n",
        "token_count = 0\n",
        "if(device == 0):\n",
        "    train_loader_length = len(train_dataloader)\n",
        "    # print(\"Total batches: \", train_loader_length)\n",
        "# print(\"Length of : \", len(train_dataloader))\n",
        "# print(\"Length of val: \", len(val_loader))\n",
        "# for  step, batch in enumerate(train_dataloader):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "_cell_guid": "5fd44e9d-01f8-4e2b-9130-4640fdab7419",
        "_uuid": "c8abd50d-6173-4aeb-8219-b6396120af54",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.145768Z",
          "iopub.status.busy": "2025-03-16T21:18:48.145554Z",
          "iopub.status.idle": "2025-03-16T21:18:48.149258Z",
          "shell.execute_reply": "2025-03-16T21:18:48.148608Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.145750Z"
        },
        "id": "R9rFUrIlpsOs",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_unused_parameters(model):\n",
        "    unused = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "\n",
        "            unused.append(name)\n",
        "    return unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "_cell_guid": "39ba72e2-ffe8-4acf-afa3-dbc3077e3e2a",
        "_uuid": "1cdf81be-4ae9-43f2-b3c5-304c73e13883",
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.182998Z",
          "iopub.status.busy": "2025-03-16T21:18:48.182799Z",
          "iopub.status.idle": "2025-03-16T21:18:48.187182Z",
          "shell.execute_reply": "2025-03-16T21:18:48.186415Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.182979Z"
        },
        "id": "AMnlZeKTpsOt",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import math\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return max_lr * (it + 1) / (warmup_iters + 1)\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "317d13ff-dc6c-4965-adfe-d28c4bd6b40d",
        "_uuid": "5feb1c4e-332c-4b97-8952-4646e07467a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.204188Z",
          "iopub.status.busy": "2025-03-16T21:18:48.203969Z",
          "iopub.status.idle": "2025-03-16T21:18:48.207464Z",
          "shell.execute_reply": "2025-03-16T21:18:48.206860Z",
          "shell.execute_reply.started": "2025-03-16T21:18:48.204168Z"
        },
        "id": "O7-thMpYpsOt",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "9704cf0c-69d0-4e34-bee6-d6f65faff9de",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "#data = next\n",
        "\n",
        "data = next(iter(train_dataloader))\n",
        "for key, value in data.items():\n",
        "    if(key == 'stop_tokens'):\n",
        "      print(value)\n",
        "      break\n",
        "\n",
        "# print(data)\n",
        "# tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "44ced50a-b954-4b04-8bc2-25b7c2601b9c",
        "_uuid": "600fec50-d0e3-4593-a316-5c25e586c4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2025-03-16T21:18:48.265086Z",
          "iopub.status.busy": "2025-03-16T21:18:48.264869Z"
        },
        "id": "nPrSPPu8cuoO",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "d7af0672-bb2a-47ce-c2e8-582c4c481fa8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "train_losses =  torch.zeros(len(train_dataloader))\n",
        "val_losses = torch.zeros(len(val_dataloader))\n",
        "wandb.init(\n",
        "    project='TTS-From-Scratch'\n",
        ")\n",
        "step = 0\n",
        "for step in tqdm(range(total_iters)):\n",
        "        # print(\"Dataloader things: \", batch)\n",
        "        # print(\"Total batches: \", len(train_dataloader))\n",
        "\n",
        "\n",
        "        # if(device == 0):\n",
        "            # if(step % 100 == 0):\n",
        "        #     if(step == train_loader_length):\n",
        "        #       break\n",
        "        print(\"Step : \", step, \"/\", total_iters)\n",
        "        print('Total batches: ', len(train_dataloader))\n",
        "        print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n",
        "                # print(\"Total tokens processed: \", token_count)\n",
        "\n",
        "        # all_gpus_avg_train_loss = None\n",
        "        # all_gpus_avg_val_loss = None\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if (step  % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
        "            losses = estimate_loss( val_dataloader, val_data_iterator, 'cuda')\n",
        "            # avg_train_loss = losses['train']\n",
        "            avg_val_loss = losses['val']\n",
        "            # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "            # if device == 0:  # Only print on main process\n",
        "            print(f\"[GPU {device}] | Step: {step} / {total_iters} | Val Loss: {losses['val']:.4f}\")\n",
        "            # print(f\"[GPU {device}] | Epoch {epoch}/{ModelArgs.epochs}| |Step: {step} | Train Loss: {losses['train']:.4f}\")\n",
        "                # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "                # Log training loss more frequently\n",
        "                # Aggregate average loss across all GPUs\n",
        "            # avg_train_loss = torch.Tensor([losses['train']]).to(device)\n",
        "            avg_val_loss = torch.Tensor([losses['val']]).to(device)\n",
        "            # torch.distributed.reduce(avg_train_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "            # torch.distributed.reduce(avg_val_loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "\n",
        "            # if device == 0:\n",
        "                # all_gpus_avg_train_loss = avg_train_loss / world_size\n",
        "                # print(f\"All_GPUs_Train_losses: {all_gpus_avg_train_loss.item():.4f}\")\n",
        "            all_gpus_avg_val_loss = avg_val_loss / world_size\n",
        "            print(f\"All_GPUs_Val_losses: {all_gpus_avg_val_loss.item():.4f}\")\n",
        "\n",
        "            # if device == 0:\n",
        "\n",
        "                # writer.add_scalar(\"All_GPUs_Train_losses\", all_gpus_avg_train_loss.item(), global_step=step)\n",
        "                # writer.add_scalar(\"All_GPUs_Val_losses\", all_gpus_avg_val_loss.item(), global_step=step)\n",
        "                # writer.add_scalar(\"training_step_loss\", losses['train'], global_step=step)\n",
        "                # writer.add_scalar(\"val_step_loss\", losses['val'], global_step=step)\n",
        "                # writer.add_scalar(\"GPU\", device, global_step=step)\n",
        "                # writer.add_scalar(\"Epoch\", epoch, global_step=step)\n",
        "\n",
        "            wandb.log({\n",
        "                    # \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
        "                    # \"All_GPUs_Train_losses\": all_gpus_avg_train_loss,\n",
        "                    \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n",
        "                    # \"training_step_loss\": losses['train'],\n",
        "                    \"val_step_loss\": losses['val'],\n",
        "                    # \"Step\": step,\n",
        "                    # \"Epoch\": epoch\n",
        "                })\n",
        "\n",
        "\n",
        "\n",
        "        #Loading a checkpoint\n",
        "        # if(os.path.exists('snapshot.pt')):\n",
        "        #    model, optimizer =  _load_snapshot(model=model, optimizer=optimizer, epoch=epoch, step=step, snapshot_path='snapshot.pt')\n",
        "\n",
        "        # if(step % save_chechpoint_iter == 0 and device == 0 and step != 0):\n",
        "\n",
        "        #     _save_snapshot(epoch=epoch, model=model, optimizer=optimizer, step=step)\n",
        "\n",
        "        if step % save_chechpoint_iter == 0 and device == 0 and step != 0:\n",
        "            print(f\"Saving the model checkpoint for step: {step}\")\n",
        "            _save_snapshot(model, optimizer, None, None, step)\n",
        "\n",
        "        accumulated_loss = 0.0\n",
        "\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        for micro_step in range(gradient_accumulation_steps):\n",
        "            try:\n",
        "                data = next(train_data_iterator)\n",
        "            except StopIteration:\n",
        "                train_data_iterator = iter(train_dataloader)\n",
        "                data = next(train_data_iterator)\n",
        "\n",
        "            data['text'] = data['text'].to(device)\n",
        "            data['input_ids'] = data['input_ids'].to(device)\n",
        "            data['labels'] = data['labels'].to(device)\n",
        "            data['stop_token'] = data['stop_tokens'].to(device)\n",
        "            idx = data['text']\n",
        "            spec = data['input_ids']\n",
        "            y = data['labels']\n",
        "            stop_token = data['stop_token']\n",
        "            # print(\n",
        "            # tgt_mask = torch.randint(1, tgt_vocab_size, (batch_size, block_size)).to(device)  #\n",
        "            # print(batch)\n",
        "            # batch = next(train_data_iterator)\n",
        "            # print(batch)\n",
        "            # batch = {k: v.to(self.local_rank) for k, v in batch.items()}\n",
        "            # idx = batch['input_ids'].to(device)\n",
        "            # idx, targets = get_batch(split='train')\n",
        "            # print(f\"Starting the train step: {step}...\")\n",
        "            # for idx, targets in train_loader:\n",
        "            # idx, targets = next(iter(train_loader))\n",
        "\n",
        "            # print(\"Idx: \", idx)\n",
        "            # print(\"Targets: \", targets)\n",
        "\n",
        "            # idx = idx.to(device)\n",
        "            # print(\"Idx: \", idx)\n",
        "            # print(\"Targets: \", targets)\n",
        "            # targets = batch['labels'].to(device)\n",
        "            # token_count += len(idx)\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "                pre, post, _stop = model(idx, spec)\n",
        "                # batch_size, block_size, embeddings_dims = stop_token.shape\n",
        "                # print(\"y: \", y.shape)\n",
        "                # print(\"Pre: \", pre.shape)\n",
        "                # print(\"post: \", post.shape)\n",
        "                # print(\"stop: \", _stop.shape)\n",
        "\n",
        "                # print(logits.shape)\n",
        "                # print(targets)\n",
        "                # stop_token = stop_token.view(batch_size*block_size, embeddings_dims)\n",
        "                # # print(\"OK\")\n",
        "                # targets = idx.view(batch_size * block_size)\n",
        "\n",
        "                # # print(\"OK2\")\n",
        "\n",
        "                pre_loss = nn.functional.mse_loss(pre, y)\n",
        "                post_loss = nn.functional.mse_loss(post, y)\n",
        "\n",
        "                # stop_token = stop_token.view(batch_size*block_size, embeddings_dims)\n",
        "                # print(\"OK\")\n",
        "                # targets = idx.view(batch_size * block_size)\n",
        "\n",
        "                # print(\"OK2\")\n",
        "\n",
        "                # pre_loss = nn.functional.mse_loss(pre, y)\n",
        "\n",
        "                stop_loss = nn.functional.binary_cross_entropy_with_logits( _stop, stop_token, pos_weight = torch.tensor([8.0]))\n",
        "                # print(pre_loss)\n",
        "                # print(post_loss)\n",
        "                # print(stop_loss)\n",
        "                loss = pre_loss + post_loss + stop_loss\n",
        "                loss = loss / gradient_accumulation_steps #IDK why div is done here specifically? Maybe think of it in terms of a very big batch being processed and there is need for equal important of each mini batch for the overall big batch\n",
        "\n",
        "                accumulated_loss += loss.detach()\n",
        "                # print(accumulated_loss)\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1) # so that we dont synchronize the gradient everytime across the GPU devices\n",
        "            scaler.scale(loss).backward()\n",
        "            # print(\"loss: \", loss.item())\n",
        "                # Check for unused parameters\n",
        "            unused_params = find_unused_parameters(model)\n",
        "            if unused_params:\n",
        "                print(f\"Unused parameters: {unused_params}\")\n",
        "        # break\n",
        "\n",
        "            # if(device == 0):\n",
        "            if(micro_step % 10 == 0):\n",
        "            #     if(step == train_loader_length):\n",
        "            #       break\n",
        "\n",
        "                    print(\"Micro Batch : \", micro_step)\n",
        "                    print(\"Step : \", step, \"/\", total_iters)\n",
        "                    print('Total batches: ', len(train_dataloader))\n",
        "                    print(\"Total gradient accumulation steps: \", gradient_accumulation_steps)\n",
        "                    print(\"Total tokens processed: \", token_count)\n",
        "            # count += 1\n",
        "\n",
        "        lr = get_lr(step)\n",
        "        for params in optimizer.param_groups:\n",
        "            params['lr'] = lr\n",
        "\n",
        "\n",
        "\n",
        "        # Compute gradient norms before clipping\n",
        "        if(clip != 0.0):\n",
        "            scaler.unscale_(optimizer) #To avoid underflow\n",
        "            total_norm_before = torch.norm(\n",
        "                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
        "            )\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "            # Compute gradient norms after clipping\n",
        "            total_norm_after = torch.norm(\n",
        "                torch.stack([torch.norm(p.grad.detach(), 2) for p in model.parameters()]), 2\n",
        "            )\n",
        "\n",
        "            if(device  == 0 and step !=0):\n",
        "                print(f\"Gradient Norm Before Clipping: {total_norm_before.item():.4f}\")\n",
        "                print(f\"Gradient Norm After Clipping: {total_norm_after.item():.4f}\")\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # optimizer.step()\n",
        "        # new_scheduler.step()\n",
        "        # print(accumulated_loss)\n",
        "        # torch.cuda.synchronize()\n",
        "        # torch.distributed.reduce(loss, dst=0, op=torch.distributed.ReduceOp.SUM)\n",
        "        # if(device == 0):\n",
        "        wandb.log({\n",
        "                    \"Learning Rate\": lr,\n",
        "                    \"All_GPUs_Train_losses\": accumulated_loss.item(),\n",
        "                    # \"All_GPUs_Val_losses\": all_gpus_avg_val_loss,\n",
        "                    # \"training_step_loss\": losses['train'],\n",
        "                    # \"val_step_loss\": losses['val'],\n",
        "                    \"Step\": step,\n",
        "                    # \"Epoch\": epoch\n",
        "\n",
        "                })\n",
        "\n",
        "\n",
        "        # model.train()\n",
        "        # wandb.log({\n",
        "        #   \"Train Loss\": train_losses.mean(),\n",
        "        #   \"Val Loss\": val_losses.mean(),\n",
        "        #   # \"epoch\": epoch\n",
        "        # })\n",
        "        # print(\"Epoch: \", epoch, \"|\", \"Train Loss: \", train_losses.mean(),  \"|\", \"Val Loss: \", val_losses.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zPJC_wsWbBy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10a6c531b06b426bb35912659a553ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1199f8b30d444865841804d062065b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43c70137107a4012add6e83bf3908284",
              "IPY_MODEL_6124f997ce4043f58545d85e5c9dadc0",
              "IPY_MODEL_96211d16cf1148519b3f6d9207e8521a"
            ],
            "layout": "IPY_MODEL_4c5d4c86a24f4c65b562b7abd923e0c5"
          }
        },
        "13c306c962f64bedb0d279a140b42594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1485d40f2cc0456da7fda87107743729": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19a6cd21c0f34ea08d312a5a9f7ef255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58564f61bfc4481c80bbf96654c4dcbd",
            "max": 2620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1485d40f2cc0456da7fda87107743729",
            "value": 2620
          }
        },
        "1b41965020b4427cb4dd1f5940c6548b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3fc312c9bbb4e039461a8e0ddd0374c",
            "max": 10480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa6360fe2bc948ef9f0e736e3fb87505",
            "value": 10480
          }
        },
        "1dafc18efead4358a80bc3789996ab33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe90efd2539741bfaacb9ceaed43e872",
              "IPY_MODEL_58b7e908d0e54832be005b9a7d0dd77f",
              "IPY_MODEL_84e1407eeb034f48b9e704f2749602d6"
            ],
            "layout": "IPY_MODEL_ddecc5d0c2194d7b97e20ba1d71ec761"
          }
        },
        "208e914165d548ca8a7a9f3a47fbab8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90df26f7d0204a7aab1917718539f5e4",
            "placeholder": "",
            "style": "IPY_MODEL_10a6c531b06b426bb35912659a553ffa",
            "value": "Flatteningtheindices:100%"
          }
        },
        "43c70137107a4012add6e83bf3908284": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a0777751f3a4ace823ec46a2cf83fb4",
            "placeholder": "",
            "style": "IPY_MODEL_f01bb9296c3549f98ca7a42a6c01e911",
            "value": "Filter:100%"
          }
        },
        "4520875846b64b15a448e89ca2641168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5d4c86a24f4c65b562b7abd923e0c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5547fead989b48749d3f3ff5d318183d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58564f61bfc4481c80bbf96654c4dcbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b7e908d0e54832be005b9a7d0dd77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d16c1b4781404bbffa4b1ac88b94b1",
            "max": 10480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_972978dd76154717aa9b260912d2fa27",
            "value": 10480
          }
        },
        "601e30955235449a8f1a280f8651a3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6124f997ce4043f58545d85e5c9dadc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76790a43f9ad440192f3b83c7755b1ad",
            "max": 2620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d18452f1acb846459e9b95e7d28c1c70",
            "value": 2620
          }
        },
        "62aadffd2d8c4122b57d14d05dfa45f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_208e914165d548ca8a7a9f3a47fbab8e",
              "IPY_MODEL_19a6cd21c0f34ea08d312a5a9f7ef255",
              "IPY_MODEL_b7f991e805ec43069443edbd3b7248b3"
            ],
            "layout": "IPY_MODEL_cba9d46844a84c5b90c94c2c444c8884"
          }
        },
        "6f00ac5858a1433d82b8bb674bb15806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5547fead989b48749d3f3ff5d318183d",
            "placeholder": "",
            "style": "IPY_MODEL_8f8b148b1cbb4f69912207102fca9963",
            "value": "Flatteningtheindices:100%"
          }
        },
        "728d3a6b84a94df58a10059e23169b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76790a43f9ad440192f3b83c7755b1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0777751f3a4ace823ec46a2cf83fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e1407eeb034f48b9e704f2749602d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68d91b41ac24242960017dd03cd3d69",
            "placeholder": "",
            "style": "IPY_MODEL_988686ba5ddb4dc08b448ea9b52c7e82",
            "value": "10480/10480[00:00&lt;00:00,116376.73examples/s]"
          }
        },
        "8f8b148b1cbb4f69912207102fca9963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "905fa0d1daab44c1981fe39b14a473de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90df26f7d0204a7aab1917718539f5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96211d16cf1148519b3f6d9207e8521a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_905fa0d1daab44c1981fe39b14a473de",
            "placeholder": "",
            "style": "IPY_MODEL_13c306c962f64bedb0d279a140b42594",
            "value": "2620/2620[00:00&lt;00:00,121300.27examples/s]"
          }
        },
        "972978dd76154717aa9b260912d2fa27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "988686ba5ddb4dc08b448ea9b52c7e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a97d1d83e9b844f6b5abf56c65e9f10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2e2a772b1ab4067a2ab4ba1705a14aa",
            "placeholder": "",
            "style": "IPY_MODEL_ad84230998594f73990b7c624c6a0108",
            "value": "10480/10480[00:02&lt;00:00,5004.53examples/s]"
          }
        },
        "ad84230998594f73990b7c624c6a0108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b68d91b41ac24242960017dd03cd3d69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f991e805ec43069443edbd3b7248b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4520875846b64b15a448e89ca2641168",
            "placeholder": "",
            "style": "IPY_MODEL_c1577849122342e0bcd8376959561965",
            "value": "2620/2620[00:00&lt;00:00,19805.56examples/s]"
          }
        },
        "c1577849122342e0bcd8376959561965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cba9d46844a84c5b90c94c2c444c8884": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d18452f1acb846459e9b95e7d28c1c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2e2a772b1ab4067a2ab4ba1705a14aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8dfab16b05042d8a652fc7e07d490b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f00ac5858a1433d82b8bb674bb15806",
              "IPY_MODEL_1b41965020b4427cb4dd1f5940c6548b",
              "IPY_MODEL_a97d1d83e9b844f6b5abf56c65e9f10d"
            ],
            "layout": "IPY_MODEL_fe19461da9f2446e8c8efa197a768e7b"
          }
        },
        "ddecc5d0c2194d7b97e20ba1d71ec761": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1d16c1b4781404bbffa4b1ac88b94b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01bb9296c3549f98ca7a42a6c01e911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3fc312c9bbb4e039461a8e0ddd0374c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6360fe2bc948ef9f0e736e3fb87505": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe19461da9f2446e8c8efa197a768e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe90efd2539741bfaacb9ceaed43e872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601e30955235449a8f1a280f8651a3ae",
            "placeholder": "",
            "style": "IPY_MODEL_728d3a6b84a94df58a10059e23169b01",
            "value": "Filter:100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
