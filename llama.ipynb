{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:39.209923Z",
     "iopub.status.busy": "2025-02-09T16:04:39.209644Z",
     "iopub.status.idle": "2025-02-09T16:04:54.183550Z",
     "shell.execute_reply": "2025-02-09T16:04:54.182842Z",
     "shell.execute_reply.started": "2025-02-09T16:04:39.209901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchtune\n",
    "!pip install torchao\n",
    "!pip install wandb\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from torchtune.modules import RMSNorm\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler \n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_secret = '...'\n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.185112Z",
     "iopub.status.busy": "2025-02-09T16:04:54.184678Z",
     "iopub.status.idle": "2025-02-09T16:04:54.189133Z",
     "shell.execute_reply": "2025-02-09T16:04:54.188427Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.185073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def setup(rank=None, world_size=None):\n",
    "    # os.environ['MASTER_ADDR'] = 'localhost' \n",
    "    # os.environ['MASTER_PORT'] = '12355'  \n",
    "    init_process_group(\"nccl\")\n",
    "\n",
    "def cleanup():\n",
    "    destroy_process_group()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.190664Z",
     "iopub.status.busy": "2025-02-09T16:04:54.190455Z",
     "iopub.status.idle": "2025-02-09T16:04:54.211999Z",
     "shell.execute_reply": "2025-02-09T16:04:54.211235Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.190648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    #Hyperparameters\n",
    "\n",
    "    block_size = 128\n",
    "    batch_size = 8\n",
    "    embeddings_dims = 768\n",
    "    attn_dropout = 0.1\n",
    "    no_of_heads = 12 #IMP needs to be thoroughly calculated\n",
    "    dropout = 0.1\n",
    "    epochs = 100\n",
    "    max_lr = 2.5e-4\n",
    "    no_of_decoder_layers = 12 #IMP needs to be thoroughly calculated\n",
    "    weight_decay_optim = 0.1\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.95\n",
    "    clip = 1.0\n",
    "    device = 'cuda'\n",
    "    no_kv_heads = 2\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.213544Z",
     "iopub.status.busy": "2025-02-09T16:04:54.213282Z",
     "iopub.status.idle": "2025-02-09T16:04:54.703819Z",
     "shell.execute_reply": "2025-02-09T16:04:54.702760Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.213515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Collab setup\n",
    "from pathlib import Path\n",
    "data_path = Path('data')\n",
    "data_path.mkdir(exist_ok=True)\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "!cp input.txt data/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.705346Z",
     "iopub.status.busy": "2025-02-09T16:04:54.705019Z",
     "iopub.status.idle": "2025-02-09T16:04:54.710210Z",
     "shell.execute_reply": "2025-02-09T16:04:54.709479Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.705321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Datasets\n",
    "\n",
    "# Using tinyshakespeare\n",
    "\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.711111Z",
     "iopub.status.busy": "2025-02-09T16:04:54.710886Z",
     "iopub.status.idle": "2025-02-09T16:04:54.730535Z",
     "shell.execute_reply": "2025-02-09T16:04:54.729613Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.711085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model):\n",
    "    ckp = model.module.state_dict()\n",
    "    torch.save(ckp, \"checkpoint.pt\")\n",
    "    print(\"Checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.731740Z",
     "iopub.status.busy": "2025-02-09T16:04:54.731529Z",
     "iopub.status.idle": "2025-02-09T16:04:54.762451Z",
     "shell.execute_reply": "2025-02-09T16:04:54.761754Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.731722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Subword level tokenization\n",
    "\n",
    "#Loading custom trained BPE\n",
    "# Load the tokenizer\n",
    "# tokenizer = Tokenizer.from_file(\"data/bpe_tokenizer_tinyshakespeare_1k.json\")\n",
    "# vocab_size = tokenizer.get_vocab_size()\n",
    "# Encode and decode functions\n",
    "# encode = lambda s: tokenizer.encode(s).ids\n",
    "# decode = lambda l: tokenizer.decode(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#Character level tokenization\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch: i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.764960Z",
     "iopub.status.busy": "2025-02-09T16:04:54.764752Z",
     "iopub.status.idle": "2025-02-09T16:04:54.975407Z",
     "shell.execute_reply": "2025-02-09T16:04:54.974561Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.764942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - ModelArgs.block_size, (ModelArgs.batch_size,))\n",
    "    x = torch.stack([data[i:i+ModelArgs.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+ModelArgs.block_size+1] for i in ix])\n",
    "    x, y = x.to(ModelArgs.device), y.to(ModelArgs.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:54.977179Z",
     "iopub.status.busy": "2025-02-09T16:04:54.976833Z",
     "iopub.status.idle": "2025-02-09T16:04:55.127900Z",
     "shell.execute_reply": "2025-02-09T16:04:55.127307Z",
     "shell.execute_reply.started": "2025-02-09T16:04:54.977150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "\n",
    "n = int(0.2 * len(encoded_data))\n",
    "train_data = encoded_data[:n]\n",
    "val_data = encoded_data[n:]\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_data, ModelArgs.block_size)\n",
    "val_dataset = TextDataset(val_data, ModelArgs.block_size)\n",
    "\n",
    "# len(val_dataset)\n",
    "val_loader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(val_loader))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.128879Z",
     "iopub.status.busy": "2025-02-09T16:04:55.128622Z",
     "iopub.status.idle": "2025-02-09T16:04:55.132859Z",
     "shell.execute_reply": "2025-02-09T16:04:55.132129Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.128850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):  \n",
    "        super().__init__()\n",
    "        self.rmsnorm_layer = RMSNorm(dim=embeddings_dims)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.rmsnorm_layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.133841Z",
     "iopub.status.busy": "2025-02-09T16:04:55.133621Z",
     "iopub.status.idle": "2025-02-09T16:04:55.153993Z",
     "shell.execute_reply": "2025-02-09T16:04:55.153202Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.133811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "class RotaryEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        batch_size: int = ModelArgs.batch_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings_dims = embeddings_dims\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.theta = 0\n",
    "        self.device=device\n",
    "\n",
    "    def apply_rope(self, seq):\n",
    "        batch_size, seq_len, embeds_dims = seq.shape\n",
    "        token_indices = torch.arange(0 , seq_len, dtype=torch.float32,  device = self.device).unsqueeze(1)\n",
    "        positions = torch.arange(0 , self.embeddings_dims, 2, dtype=torch.float32,  device = self.device).unsqueeze(0)\n",
    "        theta = 10000 ** (-2 * (positions) / self.embeddings_dims)\n",
    "        angles = token_indices * theta\n",
    "        angles = angles.expand(seq_len, -1) # because this thing needs to be applied to every sequence in the batch but with embeds dims halved\n",
    "        x_reshaped = seq.view(batch_size, seq_len, self.embeddings_dims // 2, 2)\n",
    "        \n",
    "        cos_angles = torch.cos(angles)\n",
    "        sin_angles = torch.sin(angles)\n",
    "\n",
    "\n",
    "        out = torch.stack([x_reshaped[..., 0]*cos_angles - (x_reshaped[...,1] * sin_angles), x_reshaped[...,1] * cos_angles + x_reshaped[..., 0] * sin_angles], dim=1)\n",
    "        out = out.view(batch_size, seq_len, embeds_dims)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        res = self.apply_rope(x)\n",
    "        return res \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.155149Z",
     "iopub.status.busy": "2025-02-09T16:04:55.154886Z",
     "iopub.status.idle": "2025-02-09T16:04:55.174748Z",
     "shell.execute_reply": "2025-02-09T16:04:55.173971Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.155130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RotaryAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        no_of_heads: int = ModelArgs.no_of_heads,\n",
    "        attn_dropout: int = ModelArgs.attn_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.head_size = embeddings_dims // no_of_heads\n",
    "        self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.rope = RotaryEmbeddings(embeddings_dims=self.head_size,  device = device)\n",
    "        self.dropout = nn.Dropout(p = attn_dropout)\n",
    "        self.device = device\n",
    "    def forward(self,x):\n",
    "\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "        query = self.query(x)\n",
    "   \n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "    \n",
    "        rotary_q = self.rope(query)\n",
    "        rotary_k = self.rope(key)\n",
    "        \n",
    "\n",
    "        masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "\n",
    "        weights = rotary_q.permute(2,0,1) @ rotary_k.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "        weights_masked = weights.masked_fill(masked == 0, float('-inf'))\n",
    "        scaled_weights = weights_masked / (torch.sqrt(torch.tensor(key.shape[-1])))\n",
    "        scaled_weights = F.softmax(scaled_weights, dim=-1)\n",
    "        value = scaled_weights @ values\n",
    "        out = self.dropout(value)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.175723Z",
     "iopub.status.busy": "2025-02-09T16:04:55.175541Z",
     "iopub.status.idle": "2025-02-09T16:04:55.196910Z",
     "shell.execute_reply": "2025-02-09T16:04:55.196175Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.175708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        no_of_q_heads: int,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        \n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # self.no_of_q_heads = no_of_heads // no_of_kv_heads\n",
    "        # self.no_of_q_heads = no_of_q_heads\n",
    "        self.no_of_kv_heads = 2 # I want to have a kv for each pair of query heads \n",
    "        self.head_size = embeddings_dims // no_of_q_heads\n",
    "        # self.kv_head_size = (embeddings_dims // self.no_of_kv_heads) * 2\n",
    "        self.rotary= RotaryEmbeddings(embeddings_dims=self.head_size,  device = device)\n",
    "        # self.rotary_k = RotaryEmbeddings(embeddings_dims=self.kv_head_size,  device = device)\n",
    "        # self.query = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False)\n",
    "        self.key = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.value = nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=self.head_size * self.no_of_kv_heads, out_features=embeddings_dims,  dtype=torch.float32, bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.multi_query = nn.ModuleList([nn.Linear(in_features=embeddings_dims, out_features=self.head_size,  bias=False,  device = self.device) for _ in range(self.no_of_kv_heads)])\n",
    "\n",
    "    def scaled_dot_product(self, q, k, v, block_size):\n",
    "\n",
    "            # masked = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "            q = self.rotary(q)\n",
    "            masked_table = torch.tril(torch.ones((block_size, block_size),  requires_grad=False,  device = self.device))\n",
    "            # rotary_query = matrix @ q.permute(1,2,0) # (B,T, C,C) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            # rotary_key = matrix @ k.permute(1,2,0)  #  (B,T, C,C  ) @ (B,T,C) -> (B,C,T) = (B,T,C,T)\n",
    "            # print(\"Query: \", q.shape)\n",
    "            # print(\"Keys: \", k.shape)\n",
    "            # print(q.permute(2,0,1).shape)\n",
    "            # print(k.permute(2,0,1).transpose(-2, -1).shape)\n",
    "            # weights = q.permute(2,0,1) @ k.permute(2,0,1).transpose(-2, -1)#(B,T,C,T) @ (B,T,C,T) = (T,C,C,T)\n",
    "            # weights = q @ k.permute(2,1,0)\n",
    "            # print(weights.shape)\n",
    "            # print(masked.shape)\n",
    "            weights = q @ torch.transpose(k, dim0=-2, dim1=-1) * (k.shape[-1] ** -0.5)\n",
    "            masked_values = weights.masked_fill(masked_table[: block_size, : block_size] == 0, float('-inf'))\n",
    "            weights_normalized = nn.functional.softmax(masked_values, dim=-1) #Normalize along the embeddings dimension for all the tokens\n",
    "            weights_normalized = self.dropout(weights_normalized)\n",
    "            out = weights_normalized @ v\n",
    "            return out\n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(\"MQA: \", x.shape)\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "        # query = self.query(x)\n",
    "        # matrix = self.rotary_matrix(block_size)\n",
    "\n",
    "\n",
    "        key = self.key(x)\n",
    "        values = self.value(x)\n",
    "        # print(\"Keys: \", key.shape)\n",
    "        # print(\"Values: \", values.shape)\n",
    "        # rotary_value = self.rotary(values)\n",
    "        rotary_key = self.rotary(key)\n",
    "        multi_query_concat = torch.cat([self.scaled_dot_product(query(x), rotary_key, values, block_size) for query in self.multi_query], dim=-1)\n",
    "        # print(\"Multi query: \", multi_query_concat.shape)\n",
    "\n",
    "        linear_layer= self.linear_layer(multi_query_concat)\n",
    "        # out = self.dropout(linear_layer)\n",
    "        return linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.197799Z",
     "iopub.status.busy": "2025-02-09T16:04:55.197573Z",
     "iopub.status.idle": "2025-02-09T16:04:55.218738Z",
     "shell.execute_reply": "2025-02-09T16:04:55.217977Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.197771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "         device,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        # no_of_q_heads: int = ModelArgs.no_of_heads,\n",
    "        mqa_heads: int = ModelArgs.no_kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.no_of_kv_heads = no_of_kv_heads\n",
    "        self.no_of_q_heads = ModelArgs.no_of_heads // mqa_heads\n",
    "        # self.head_dim = embeddings_dims // self.no_kv_heads\n",
    "        self.dropout = nn.Dropout(p = ModelArgs.attn_dropout)\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims * self.no_of_q_heads, out_features=embeddings_dims , dtype=torch.float32,  bias=False,  device = device)\n",
    "        self.device = device\n",
    "        self.mqa = nn.ModuleList([MQA(no_of_q_heads=self.no_of_q_heads, embeddings_dims=embeddings_dims, device = self.device, block_size=block_size) for _ in range(self.no_of_q_heads)])\n",
    "        # self.mqa = MQA(no_of_q_heads=self.no_of_q_heads, device=self.device, embeddings_dims=embeddings_dims, block_size=block_size)\n",
    "    def forward(self,x):\n",
    "\n",
    "        batch, block_size, embeddings_dims = x.shape\n",
    "\n",
    "        # res = self.mqa(x)\n",
    "        grouped_query_concat = torch.cat([group(x) for group in self.mqa], dim=-1)\n",
    "\n",
    "        linear_layer= self.linear_layer(grouped_query_concat) #Basically MQA is made into GQA with no_of_q_heads and this class right here is just to consolidate everything into one\n",
    "        out = self.dropout(linear_layer)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.219733Z",
     "iopub.status.busy": "2025-02-09T16:04:55.219523Z",
     "iopub.status.idle": "2025-02-09T16:04:55.234380Z",
     "shell.execute_reply": "2025-02-09T16:04:55.233770Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.219713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = x * self.sig(x)\n",
    "\n",
    "        return swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.235427Z",
     "iopub.status.busy": "2025-02-09T16:04:55.235205Z",
     "iopub.status.idle": "2025-02-09T16:04:55.259121Z",
     "shell.execute_reply": "2025-02-09T16:04:55.258438Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.235398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SWiGLU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        block_size: int = ModelArgs.block_size,\n",
    "        embeddings_dims: int = ModelArgs.embeddings_dims\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dims = int(2 * ( 4 * embeddings_dims) / 3)\n",
    "        self.swish = Swish(block_size=block_size, embeddings_dims=embeddings_dims, device=device)\n",
    "        self.linear_layer1 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer2 = nn.Linear(in_features=embeddings_dims, out_features=self.hidden_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "        self.linear_layer3 = nn.Linear(in_features=self.hidden_dims, out_features=embeddings_dims,  bias=False, dtype=torch.float32,  device = device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_res = self.swish(self.linear_layer1(x))\n",
    "        x_V = self.linear_layer2(x)\n",
    "        res = torch.mul(swish_res, x_V)\n",
    "        out = self.linear_layer3(res)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.260140Z",
     "iopub.status.busy": "2025-02-09T16:04:55.259842Z",
     "iopub.status.idle": "2025-02-09T16:04:55.279076Z",
     "shell.execute_reply": "2025-02-09T16:04:55.278429Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.260114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                   dropout = ModelArgs.dropout\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.swiglue = SWiGLU(block_size=block_size, embeddings_dims=embeddings_dims,  device = device)\n",
    "        # self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.swiglue(x)\n",
    "        # x = self.linear_layer(x)\n",
    "        # x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.280074Z",
     "iopub.status.busy": "2025-02-09T16:04:55.279790Z",
     "iopub.status.idle": "2025-02-09T16:04:55.299800Z",
     "shell.execute_reply": "2025-02-09T16:04:55.299243Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.280046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                  device,\n",
    "                embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                dropout = ModelArgs.dropout,\n",
    "                block_size: int = ModelArgs.block_size,\n",
    "                vocab_size: int = ModelArgs.vocab_size,\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.feedforward_network = FFN(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size,  device = device)\n",
    "        self.gqa = GQA(embeddings_dims=embeddings_dims, block_size=block_size, mqa_heads=2,  device = device)\n",
    "        # self.norm = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm1 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.norm2 = Normalization(embeddings_dims=embeddings_dims)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.gqa(self.norm1(x))\n",
    "        x = x + self.feedforward_network(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.300725Z",
     "iopub.status.busy": "2025-02-09T16:04:55.300520Z",
     "iopub.status.idle": "2025-02-09T16:04:55.318755Z",
     "shell.execute_reply": "2025-02-09T16:04:55.318109Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.300681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self,\n",
    "                device,\n",
    "                  embeddings_dims: int = ModelArgs.embeddings_dims,\n",
    "                  no_of_decoder_layers: int = ModelArgs.no_of_decoder_layers,\n",
    "                  block_size: int = ModelArgs.block_size,\n",
    "                  vocab_size: int = ModelArgs.vocab_size,\n",
    "                  dropout = ModelArgs.dropout\n",
    "\n",
    "                 ) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_dims,  dtype=torch.float32,  device = device)\n",
    "        self.decoder = nn.Sequential(*[DecoderLayer(embeddings_dims=embeddings_dims, block_size=block_size, vocab_size=vocab_size, dropout=dropout,  device = device) for _ in range(no_of_decoder_layers)])\n",
    "        self.linear_layer = nn.Linear(in_features=embeddings_dims, out_features=vocab_size,  dtype=torch.float32,  device = device)\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        # self.norm = Normalization(embeddings_dims)\n",
    "        \n",
    "        \n",
    "        #weight tying\n",
    "        # self.embeddings.weight = self.linear_layer.weight\n",
    "    \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "               \n",
    "                     \n",
    "                    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.decoder(x)\n",
    "        # x = self.norm(x)\n",
    "        x = self.linear_layer(x)\n",
    "        # out = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.319579Z",
     "iopub.status.busy": "2025-02-09T16:04:55.319370Z",
     "iopub.status.idle": "2025-02-09T16:04:55.343462Z",
     "shell.execute_reply": "2025-02-09T16:04:55.342836Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.319563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#fetching the multi gpu device ids\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:55.344336Z",
     "iopub.status.busy": "2025-02-09T16:04:55.344151Z",
     "iopub.status.idle": "2025-02-09T16:04:55.599762Z",
     "shell.execute_reply": "2025-02-09T16:04:55.599097Z",
     "shell.execute_reply.started": "2025-02-09T16:04:55.344320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# ModelArgs.device = device\n",
    "model = Llama(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout)\n",
    "# model = model.to(ModelArgs.device)\n",
    "\n",
    "# model = DDP(model, device_ids=[gpu_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T14:41:17.545685Z",
     "iopub.status.busy": "2025-02-09T14:41:17.545482Z",
     "iopub.status.idle": "2025-02-09T14:41:21.596059Z",
     "shell.execute_reply": "2025-02-09T14:41:21.595134Z",
     "shell.execute_reply.started": "2025-02-09T14:41:17.545667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Printing a summary of the architecture\n",
    "!pip install torchinfo \n",
    "\n",
    "from torchinfo import summary\n",
    "idx, targets = get_batch('test')\n",
    "idx = idx.to(ModelArgs.device)\n",
    "summary(model=model,\n",
    "        input_data=idx,\n",
    "        # input_size=(ModelArgs.batch_size, ModelArgs.block_size, ModelArgs.embeddings_dims),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:04:58.211771Z",
     "iopub.status.busy": "2025-02-09T16:04:58.211500Z",
     "iopub.status.idle": "2025-02-09T16:04:58.221497Z",
     "shell.execute_reply": "2025-02-09T16:04:58.220473Z",
     "shell.execute_reply.started": "2025-02-09T16:04:58.211751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    setup()\n",
    "    device=torch.distributed.get_rank()\n",
    "    \n",
    "    \n",
    "    # rank = torch.distributed.get_rank()\n",
    "    print(f\"Start running basic DDP example on rank {device}.\")\n",
    "    # # create model and move it to GPU with id rank\n",
    "    # device_id = rank % torch.cuda.device_count()\n",
    "    # CFG = ModelArgs()\n",
    "    \n",
    "    if(device == 0):\n",
    "        # Initialise run\n",
    "        wandb.init(\n",
    "            # entity = 'rajceo2031',\n",
    "                        project = 'Llama-DDP',\n",
    "                        # config = CFG,\n",
    "                        # save_code = True,\n",
    "                        #group = 'ANN',\n",
    "                        #job_type = 'train'\n",
    ")\n",
    "    \n",
    "    model = Llama(embeddings_dims=ModelArgs.embeddings_dims, block_size=ModelArgs.block_size, vocab_size=ModelArgs.vocab_size, dropout=ModelArgs.dropout, device=device)\n",
    "    # Optimizer setup and scheduler steup\n",
    "    torch.cuda.set_device(device)\n",
    "    model = model.cuda()\n",
    "        \n",
    "    # Wrap model with DDP after moving to GPU\n",
    "    model = DDP(model, find_unused_parameters=False)\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=ModelArgs.max_lr)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=ModelArgs.batch_size, shuffle=False, sampler = DistributedSampler(val_dataset))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=ModelArgs.batch_size, shuffle=False, sampler = DistributedSampler(train_dataset))\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "    save_chechpoint_iter = 100\n",
    "    total_iters = 25000\n",
    "    eval_iters = 100\n",
    "    # for X,y in train_loader:\n",
    "    #     print(X.shape)\n",
    "    #     print(y.shape)\n",
    "\n",
    "     # Only create progress bar for rank 0\n",
    "    # eval_epoch_iterator = range(eval_iters)\n",
    "    train_epoch_iterator = range(total_iters)\n",
    "    if device == 0:\n",
    "        train_epoch_iterator = tqdm(train_epoch_iterator, desc=\"Training\")\n",
    "        # eval_epoch_iterator = tqdm(eval_epoch_iterator, desc='Validation')\n",
    "   \n",
    "    # lr_scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max= total_steps - initial_iters)\n",
    "    val_iter = iter(loader)\n",
    "    @torch.inference_mode()\n",
    "    def estimate_loss():\n",
    "        out = {}\n",
    "        \n",
    "        model.eval()\n",
    "        loader = None\n",
    "        # print(\"Starting the eval...\")\n",
    "        for split in ['train', 'val']:\n",
    "            print(f\"Starting with {split} evaluation...\")\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                # idx, targets = get_batch(split=split)\n",
    "                if(split == 'train'):\n",
    "                    loader = train_loader\n",
    "                else:\n",
    "                    loader = val_loader\n",
    "                    \n",
    "                # for idx, targets in loader:\n",
    "                idx, targets = next(val_iter)\n",
    "                idx = idx.cuda(non_blocking=True)\n",
    "                targets = targets.cuda(non_blocking=True)\n",
    "                logits = model(idx)\n",
    "                batch_size, block_size, embeddings_dims = logits.shape\n",
    "                logits = logits.view(batch_size*block_size, embeddings_dims) # Total tokens(words) => batch_size * block_size\n",
    "                targets = targets.view(batch_size * block_size)\n",
    "                loss = nn.functional.cross_entropy(logits, targets)\n",
    "                losses[k] = loss.item()\n",
    "\n",
    "                # if device == 0:\n",
    "                #     eval_epoch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "                    \n",
    "            out[split] = losses.mean()\n",
    "            \n",
    "\n",
    "        model.train()\n",
    "        return out\n",
    "\n",
    "    # model = model.to(rank)\n",
    "    model.train()\n",
    "    iterator = iter(train_loader)\n",
    "    # for step in tqdm(range(total_iters)):\n",
    "    for step in train_epoch_iterator:\n",
    "        train_loader.sampler.set_epoch(step)\n",
    "        val_loader.sampler.set_epoch(step)\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if (step  % eval_iters == 0 and step != 0) or step == total_iters - 1:\n",
    "            losses = estimate_loss()\n",
    "            # print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            if device == 0:  # Only print on main process\n",
    "                print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "                # Log training loss more frequently\n",
    "        # if device == 0:\n",
    "                wandb.log({\n",
    "                    \"training_step_loss\": losses['train'],\n",
    "                    \"val_step_loss\": losses['val'],\n",
    "                    \"step\": step\n",
    "                })\n",
    "        if(step % save_chechpoint_iter == 0 and device == 0 and step != 0):\n",
    "            print(f\"Saving the model checkpoint for step: {step}\")\n",
    "            save_checkpoint(model)\n",
    "        \n",
    "        \n",
    "       \n",
    "        # idx, targets = get_batch(split='train')\n",
    "        # print(f\"Starting the train step: {step}...\")\n",
    "        # for idx, targets in train_loader:\n",
    "        idx, targets = next(iterator)\n",
    "        idx = idx.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "        logits = model(idx)\n",
    "        batch_size, block_size, embeddings_dims = logits.shape\n",
    "        logits = logits.view(batch_size*block_size, embeddings_dims)\n",
    "        targets = targets.view(batch_size * block_size)\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if device == 0:\n",
    "        #     train_epoch_iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        # print(loss.item())\n",
    "        # break\n",
    "\n",
    "        # if step != 0 and (step % eval_iters == 0 or step == total_steps -1) :\n",
    "        #     loss_values = estimate_loss()\n",
    "        #     print(\"Train Loss at {} steps : {}\".format(step, loss.item()), \"Val Loss at {} steps : {}\".format(step, loss_values['val']))\n",
    "\n",
    "        # Add after a training step:\n",
    "        # unused_params = find_unused_parameters(model)\n",
    "        # print(\"Unused parameters:\", unused_params)\n",
    "        # break\n",
    "        \n",
    "\n",
    "    # Cleanup\n",
    "    if device == 0:\n",
    "        wandb.finish()\n",
    "    cleanup()\n",
    "\n",
    "    \n",
    "# %% [code] {\"execution\":{\"iopub.execute_input\":\"2025-02-09T16:05:17.608500Z\",\"iopub.status.busy\":\"2025-02-09T16:05:17.608220Z\",\"iopub.status.idle\":\"2025-02-09T16:05:19.612034Z\",\"shell.execute_reply\":\"2025-02-09T16:05:19.610871Z\",\"shell.execute_reply.started\":\"2025-02-09T16:05:17.608481Z\"}}\n",
    "world_size = torch.cuda.device_count()\n",
    "print(f\"World size: {world_size}\")\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T16:05:17.608500Z",
     "iopub.status.busy": "2025-02-09T16:05:17.608220Z",
     "iopub.status.idle": "2025-02-09T16:05:19.612034Z",
     "shell.execute_reply": "2025-02-09T16:05:19.610871Z",
     "shell.execute_reply.started": "2025-02-09T16:05:17.608481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "print(f\"World size: {world_size}\")\n",
    "train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train(1, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
